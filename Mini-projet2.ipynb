{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import FloatTensor\n",
    "\n",
    "NAME_INDEX = 0\n",
    "DATA_INDEX = 1\n",
    "GRAD_INDEX = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_disc_set(nb):\n",
    "    # Generates data and corresponding target\n",
    "    \n",
    "    input = FloatTensor(nb, 2).uniform_(0, 1)\n",
    "    target = torch.norm(input, 2, 1) < math.sqrt(1/(2*math.pi))\n",
    "    return input, target.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter():\n",
    "    # class used to create all the parameters in the same way\n",
    "    \n",
    "    def __init__(self, name, tensor, gradient):\n",
    "        self.name = name\n",
    "        self.data = tensor\n",
    "        self.grad = gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module ( Parameter ) :\n",
    "    # base module that the following class inherits\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Parameter, self).__init__()\n",
    "        self.param = []\n",
    "        \n",
    "    def forward (self, * input) :\n",
    "        # Computes the forward pass of the Module. Need to be implemented in future class if one wants to use it.\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward (self , * gradwrtoutput) :\n",
    "        # Computes the backward pass of the Module. Need to be implemented in future class if one wants to use it.\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def init_parameters ( self ):\n",
    "        # Initialize the proper parameters for the Module. Need to be implemented in future class if one wants to use it.\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def add_parameter(self, parameter):\n",
    "        # Adds the input parameter into\n",
    "        if parameter.__class__.__name__ == 'Parameter':\n",
    "            self.param.append((parameter.name, parameter.data, parameter.grad))\n",
    "        elif parameter.__class__.__name__ == 'list':                        \n",
    "            if parameter != []:\n",
    "                self.param.append(parameter)\n",
    "                    \n",
    "    def zero_grad( self ):\n",
    "        for i in range(len(self.param)):\n",
    "            for j in range(len(self.param[i])):\n",
    "                self.param[i][j][GRAD_INDEX][:] = 0\n",
    "                    \n",
    "    def optimizer (self, lr = 1e-2):\n",
    "        for i in range(len(self.param)):\n",
    "            for j in range(len(self.param[i])):\n",
    "                self.param[i][j][DATA_INDEX][:] -= lr * self.param[i][j][GRAD_INDEX][:]\n",
    "                \n",
    "    def parameters ( self ):\n",
    "        return self.param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module for MSEloss\n",
    "\n",
    "class MSEloss( Module ):\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MSEloss, self).__init__()\n",
    "    def forward (self, input, target):\n",
    "        return input.sub(target.view(-1, 1)).pow(2).sum()    \n",
    "    def backward (self, input, target):\n",
    "        return 2*(input.sub(target.view(-1, 1)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module ReLU\n",
    "\n",
    "class ReLU( Module ):\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "        self.name = 'ReLU'\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return input.clamp(min = 0)\n",
    "    \n",
    "    def backward(self, input, dx):\n",
    "        input[input <= 0] = 0\n",
    "        input[input > 0] = 1\n",
    "        return input * dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module Tanh\n",
    "\n",
    "class Tanh( Module ):\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "        self.name = 'Tanh'\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return input.tanh()\n",
    "    \n",
    "    def backward(self, input, dx):\n",
    "        return 4 * (input.exp() + input.mul(-1).exp()).pow(-2) * dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module linear\n",
    "\n",
    "class Linear( Module ):\n",
    "\n",
    "    \n",
    "    Linear_counter = 0\n",
    "    \n",
    "    def __init__(self, input_features, output_features, eps = 1e-2):\n",
    "        super(Linear, self).__init__()\n",
    "        self.name = 'Linear'\n",
    "        Linear.Linear_counter += 1\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "        \n",
    "        self.init_parameters(input_features, output_features, eps)\n",
    "        \n",
    "    def init_parameters (self, input_features, output_features, eps):\n",
    "        weigths_name = f'weights{self.Linear_counter}'\n",
    "        bias_name = f'bias{self.Linear_counter}'\n",
    "        self.weights = Parameter(weigths_name, torch.Tensor(input_features, output_features),\n",
    "                                 torch.Tensor(input_features, output_features))\n",
    "        self.bias = Parameter(bias_name, torch.Tensor(output_features), torch.Tensor(output_features))\n",
    "        self.weights.data.normal_(0, eps)\n",
    "        self.weights.grad.zero_()\n",
    "        self.bias.grad.zero_()\n",
    "        self.bias.data.normal_(0, eps)\n",
    "        \n",
    "        self.add_parameter(self.weights)\n",
    "        self.add_parameter(self.bias)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = input.mm(self.weights.data)\n",
    "        output += self.bias.data\n",
    "        return output\n",
    "    \n",
    "    def backward(self, input, ds):\n",
    "        dx = ds.mm(self.weights.data.t())\n",
    "        dw = input.t().mm(ds)\n",
    "        db = ds.t().mm(torch.ones(ds.size(0), 1))\n",
    "        \n",
    "        self.weights.grad.add_(dw)\n",
    "        self.bias.grad.add_(db)\n",
    "        return dx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module Sequential\n",
    "\n",
    "class Sequential( Module ):\n",
    "    \n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        super(Sequential, self).__init__()\n",
    "        Linear.Linear_counter = 0\n",
    "        self.module_nb = len(args)\n",
    "        self.fc = [None] * self.module_nb\n",
    "        self.x = [None] * (self.module_nb + 1)\n",
    "        \n",
    "        for id, module in enumerate(args):\n",
    "            self.fc[id] = module\n",
    "            self.init_parameters(id)\n",
    "        \n",
    "    def init_parameters (self, id):\n",
    "        self.add_parameter(self.fc[id].parameters())\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.x[0] = input\n",
    "        for i in range(1, self.module_nb + 1):\n",
    "            self.x[i] = self.fc[i-1].forward(self.x[i-1])\n",
    "        return self.x[self.module_nb]\n",
    "    \n",
    "    def backward(self, dloss):\n",
    "        dx = [None] * (self.module_nb + 1)\n",
    "        dx[self.module_nb] = dloss\n",
    "        for i in range(1, self.module_nb + 1):\n",
    "            j = self.module_nb - i\n",
    "            dx[j] = self.fc[j].backward(self.x[j], dx[j+1])\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net 2 layers\n",
    "\n",
    "Linear.Linear_counter = 0\n",
    "\n",
    "class Net2( Module ):\n",
    "    def __init__( self , hidden_layer):\n",
    "        super(Net2, self).__init__()\n",
    "        self.fc1 = Linear(2, hidden_layer)\n",
    "        self.fc2 = ReLU()\n",
    "        self.fc3 = Linear(hidden_layer, 1)\n",
    "        self.init_parameters()\n",
    "        \n",
    "    def init_parameters ( self ):\n",
    "        self.add_parameter( self.fc1.parameters() )\n",
    "        self.add_parameter( self.fc2.parameters() )\n",
    "        self.add_parameter( self.fc3.parameters() )\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.x0 = input\n",
    "        s1 = self.fc1.forward( input )\n",
    "        self.s1 = s1\n",
    "        x1 = self.fc2.forward( s1 )\n",
    "        self.x1 = x1\n",
    "        s2 = self.fc3.forward( x1 )\n",
    "        self.s2 = s2\n",
    "        x2 = self.fc2.forward( s2 )\n",
    "        self.x2 = x2\n",
    "        return x2\n",
    "    \n",
    "    def backward(self, dloss):\n",
    "        dx2 = dloss\n",
    "        ds2 = self.fc2.backward(self.s2) * dx2\n",
    "        dx1, dw3, db3 = self.fc3.backward( ds2 , self.x1 )\n",
    "        self.fc3.weights.grad.add_(dw3)\n",
    "        self.fc3.bias.grad.add_(db3)\n",
    "        \n",
    "        ds1 = self.fc2.backward(self.s1) * dx1\n",
    "        \n",
    "        dx0, dw1, db1 = self.fc1.backward( ds1 , self.x0 )\n",
    "\n",
    "        self.fc1.weights.grad.add_(dw1)\n",
    "        self.fc1.bias.grad.add_(db1)\n",
    "        \n",
    "        return dx0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net 4 layers\n",
    "\n",
    "Linear.Linear_counter = 0\n",
    "\n",
    "class Net4( Module ):\n",
    "    def __init__(self, hidden_layer):\n",
    "        super(Net4, self).__init__()\n",
    "        self.fc1 = Linear(2, 2*hidden_layer)\n",
    "        self.fc2 = ReLU()\n",
    "        self.fc3 = Linear(2*hidden_layer, hidden_layer)\n",
    "        self.fc4 = Linear(hidden_layer, 1)\n",
    "        self.init_parameters()\n",
    "        \n",
    "    def init_parameters ( self ):\n",
    "        self.add_parameter( self.fc1.parameters() )\n",
    "        self.add_parameter( self.fc2.parameters() )\n",
    "        self.add_parameter( self.fc3.parameters() )\n",
    "        self.add_parameter( self.fc4.parameters() )\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x0 = x\n",
    "        x = self.fc1.forward( x )\n",
    "        self.s1 = x\n",
    "        x = self.fc2.forward( x )\n",
    "        self.x1 = x\n",
    "        x = self.fc3.forward( x )\n",
    "        self.s2 = x\n",
    "        x = self.fc2.forward( x )\n",
    "        self.x2 = x\n",
    "        x = self.fc4.forward( x )\n",
    "        self.s3 = x\n",
    "        x = self.fc2.forward( x )\n",
    "        self.x3 = x\n",
    "        return x\n",
    "    \n",
    "    def backward(self, dloss):\n",
    "        dx3 = dloss\n",
    "        ds3 = self.fc2.backward(self.s3) * dx3\n",
    "        dx2, dw4, db4 = self.fc4.backward( ds3, self.x2 )\n",
    "        self.fc4.weights.grad.add_(dw4)\n",
    "        self.fc4.bias.grad.add_(db4)\n",
    "        ds2 = self.fc2.backward(self.s2) * dx2\n",
    "        dx1, dw3, db3 = self.fc3.backward( ds2 , self.x1 )\n",
    "        self.fc3.weights.grad.add_(dw3)\n",
    "        self.fc3.bias.grad.add_(db3)\n",
    "        \n",
    "        ds1 = self.fc2.backward(self.s1) * dx1\n",
    "        \n",
    "        dx0, dw1, db1 = self.fc1.backward( ds1 , self.x0 )\n",
    "\n",
    "        self.fc1.weights.grad.add_(dw1)\n",
    "        self.fc1.bias.grad.add_(db1)\n",
    "        \n",
    "        return dx0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, mini_batch_size, lr = 1e-1):\n",
    "    nb_epochs = 250\n",
    "    criterion = MSEloss()\n",
    "    \n",
    "    for e in range(0, nb_epochs):\n",
    "        sum_loss = 0\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model.forward(train_input.narrow(0, b, mini_batch_size))\n",
    "            loss = criterion.forward(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            dloss = criterion.backward(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            sum_loss += loss\n",
    "            model.zero_grad()\n",
    "            dx0 = model.backward(dloss)\n",
    "            model.optimizer(lr)\n",
    "        print(e, sum_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, data_input, data_target, mini_batch_size):\n",
    "    total_nb_errors = 0\n",
    "    \n",
    "    for b in range(0, data_input.size(0), mini_batch_size):\n",
    "        output = model.forward(data_input.narrow(0, b, mini_batch_size))\n",
    "        _, predicted_classes = torch.max(output, 1)\n",
    "        for k in range(0, mini_batch_size):\n",
    "            if data_target[b + k] != predicted_classes[k]:\n",
    "                total_nb_errors += 1\n",
    "\n",
    "    return total_nb_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dave/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: other is not broadcastable to self, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 116645.25551211373\n",
      "1 41331.8831437583\n",
      "2 18021.34329176069\n",
      "3 9138.0775416703\n",
      "4 5290.15911744412\n",
      "5 3456.256001225327\n",
      "6 2468.6180838231357\n",
      "7 1871.310609977581\n",
      "8 1484.8518865022306\n",
      "9 1219.83925419473\n",
      "10 1030.311244781472\n",
      "11 888.8687953189947\n",
      "12 781.1371486384669\n",
      "13 697.8410695310886\n",
      "14 632.3934413792165\n",
      "15 579.8927448530703\n",
      "16 537.0246050404166\n",
      "17 501.99412862592\n",
      "18 472.6186490739483\n",
      "19 447.31351964498754\n",
      "20 425.6074293126985\n",
      "21 407.09796482491856\n",
      "22 391.35629266983597\n",
      "23 377.87908494478324\n",
      "24 366.278015550497\n",
      "25 356.2991072088007\n",
      "26 347.7019974943871\n",
      "27 340.24796971798787\n",
      "28 333.7291866251262\n",
      "29 328.01460093572905\n",
      "30 322.9825093895779\n",
      "31 318.53136855485536\n",
      "32 314.577081879444\n",
      "33 311.0467675074815\n",
      "34 307.8811345961585\n",
      "35 305.02617909654737\n",
      "36 302.44528227770934\n",
      "37 300.10690873287695\n",
      "38 297.9803182618971\n",
      "39 296.03720269899713\n",
      "40 294.25650983152383\n",
      "41 292.62036053999327\n",
      "42 291.1137366568728\n",
      "43 289.7253091671664\n",
      "44 288.44233204540797\n",
      "45 287.2549022397434\n",
      "46 286.1552996047394\n",
      "47 285.1351163955551\n",
      "48 284.1869522710349\n",
      "49 283.3051059811696\n",
      "50 282.4846471569035\n",
      "51 281.72089862611847\n",
      "52 281.0096051385335\n",
      "53 280.34707313109993\n",
      "54 279.72897949864375\n",
      "55 279.1506733524295\n",
      "56 278.60917138206196\n",
      "57 278.10216380293423\n",
      "58 277.627069544762\n",
      "59 277.1816976867767\n",
      "60 276.76411893109616\n",
      "61 276.3725597972516\n",
      "62 276.0052257323896\n",
      "63 275.6605536999414\n",
      "64 275.3370078827138\n",
      "65 275.0330768498243\n",
      "66 274.7473345123726\n",
      "67 274.4790051883283\n",
      "68 274.2267858428077\n",
      "69 273.9888993591651\n",
      "70 273.7641272011474\n",
      "71 273.5516113326531\n",
      "72 273.35007317083364\n",
      "73 273.15861784189474\n",
      "74 272.9766737521859\n",
      "75 272.8037744083849\n",
      "76 272.63938659764244\n",
      "77 272.48309471235325\n",
      "78 272.334367534233\n",
      "79 272.1926068047469\n",
      "80 272.05736419212553\n",
      "81 271.92815123410537\n",
      "82 271.80470617482206\n",
      "83 271.68677233760536\n",
      "84 271.57374244726816\n",
      "85 271.46568074682546\n",
      "86 271.3624206147488\n",
      "87 271.2636716154884\n",
      "88 271.1692280345651\n",
      "89 271.07883457111893\n",
      "90 270.9922231399978\n",
      "91 270.90918847324065\n",
      "92 270.8295397059903\n",
      "93 270.7531887603982\n",
      "94 270.6799146633857\n",
      "95 270.6095626760871\n",
      "96 270.54203500143285\n",
      "97 270.4771880022454\n",
      "98 270.41484586944534\n",
      "99 270.3547919722041\n",
      "100 270.2969036866416\n",
      "101 270.24108620411425\n",
      "102 270.18723833084186\n",
      "103 270.1352885586603\n",
      "104 270.08515723659366\n",
      "105 270.0367152469553\n",
      "106 269.98994490719144\n",
      "107 269.9447115153598\n",
      "108 269.9009663476463\n",
      "109 269.85863756152685\n",
      "110 269.8177144068104\n",
      "111 269.77815554752397\n",
      "112 269.7398619836554\n",
      "113 269.70281509964843\n",
      "114 269.6669499151467\n",
      "115 269.63221835464356\n",
      "116 269.59859446996415\n",
      "117 269.56604332658753\n",
      "118 269.53452747037954\n",
      "119 269.5039983505849\n",
      "120 269.4744242811912\n",
      "121 269.4457863501294\n",
      "122 269.418068241961\n",
      "123 269.39118637544743\n",
      "124 269.3651143249008\n",
      "125 269.3398265221622\n",
      "126 269.3152740159421\n",
      "127 269.2914385353797\n",
      "128 269.26826470164815\n",
      "129 269.2457608075201\n",
      "130 269.22390893231204\n",
      "131 269.20266495009855\n",
      "132 269.1820292139346\n",
      "133 269.16199097807566\n",
      "134 269.14252289422166\n",
      "135 269.12355888373713\n",
      "136 269.10509200634215\n",
      "137 269.08710779377503\n",
      "138 269.0695597101003\n",
      "139 269.0524339752901\n",
      "140 269.0357093100231\n",
      "141 269.01938650765806\n",
      "142 269.00340108341334\n",
      "143 268.9877573574122\n",
      "144 268.9724313274928\n",
      "145 268.9574429041422\n",
      "146 268.9427644216157\n",
      "147 268.92838405417206\n",
      "148 268.9143083327682\n",
      "149 268.9005154101178\n",
      "150 268.8869962733006\n",
      "151 268.8737512368243\n",
      "152 268.8607860711636\n",
      "153 268.84808279026765\n",
      "154 268.8356209885096\n",
      "155 268.8234058848466\n",
      "156 268.81143506307853\n",
      "157 268.7997178826481\n",
      "158 268.7882254807628\n",
      "159 268.77697975572664\n",
      "160 268.7659438913106\n",
      "161 268.7551426925347\n",
      "162 268.7445530666155\n",
      "163 268.73417602776317\n",
      "164 268.72400686890614\n",
      "165 268.71403245956026\n",
      "166 268.70426336128\n",
      "167 268.6947008893949\n",
      "168 268.685314288241\n",
      "169 268.6761179808673\n",
      "170 268.6670939193573\n",
      "171 268.6582330025267\n",
      "172 268.6495522534242\n",
      "173 268.6410315101384\n",
      "174 268.6326698680641\n",
      "175 268.6244597553741\n",
      "176 268.61639830627246\n",
      "177 268.60848931473447\n",
      "178 268.60073742247187\n",
      "179 268.59312761799083\n",
      "180 268.58565746454406\n",
      "181 268.57833290015697\n",
      "182 268.5711416438862\n",
      "183 268.5640866613103\n",
      "184 268.55715608729224\n",
      "185 268.55035607946047\n",
      "186 268.5436827188314\n",
      "187 268.53713042371237\n",
      "188 268.5307084175947\n",
      "189 268.5244032100836\n",
      "190 268.51821618736085\n",
      "191 268.5121446739297\n",
      "192 268.50618502403563\n",
      "193 268.5003259062803\n",
      "194 268.49456540510823\n",
      "195 268.4889097979758\n",
      "196 268.4833335484145\n",
      "197 268.4778419005452\n",
      "198 268.47243779408745\n",
      "199 268.4671211800305\n",
      "200 268.46189068374224\n",
      "201 268.4567409034353\n",
      "202 268.45167266367935\n",
      "203 268.44668725458905\n",
      "204 268.44178332190495\n",
      "205 268.43695468630176\n",
      "206 268.4322036196827\n",
      "207 268.4275309328805\n",
      "208 268.4229227672331\n",
      "209 268.41839006874943\n",
      "210 268.41393012943445\n",
      "211 268.4095427574357\n",
      "212 268.4052302604541\n",
      "213 268.4009877064382\n",
      "214 268.3968122942024\n",
      "215 268.39270694873994\n",
      "216 268.3886648173793\n",
      "217 268.3846856771852\n",
      "218 268.380764063535\n",
      "219 268.3769043555658\n",
      "220 268.373104882412\n",
      "221 268.3693617115787\n",
      "222 268.3656776549833\n",
      "223 268.3620559893243\n",
      "224 268.35849291101476\n",
      "225 268.35498055635253\n",
      "226 268.3515240375018\n",
      "227 268.34812813344615\n",
      "228 268.3447890701518\n",
      "229 268.34149996836186\n",
      "230 268.3382581219846\n",
      "231 268.335074467768\n",
      "232 268.3319337505345\n",
      "233 268.3288387415432\n",
      "234 268.32579065648315\n",
      "235 268.32278639238575\n",
      "236 268.3198199473695\n",
      "237 268.3168932533432\n",
      "238 268.3140081930056\n",
      "239 268.31116726770415\n",
      "240 268.3083651262059\n",
      "241 268.30559915334015\n",
      "242 268.30286855941813\n",
      "243 268.3001713720332\n",
      "244 268.29751109564677\n",
      "245 268.29488628965737\n",
      "246 268.2922958027084\n",
      "247 268.2897348531187\n",
      "248 268.28721166970854\n",
      "249 268.28471867775215\n",
      "130\n",
      "test error Net 13.00%% 130/1000\n"
     ]
    }
   ],
   "source": [
    "train_input, train_target = generate_disc_set(1000)\n",
    "test_input, test_target = generate_disc_set(1000)\n",
    "mean, std = train_input.mean(), train_input.std()\n",
    "train_input.sub_(mean).div_(std)\n",
    "test_input.sub_(mean).div_(std);\n",
    "\n",
    "mini_batch_size = 100\n",
    "\n",
    "model = Sequential(Linear(2, 25, 1), ReLU(), Linear(25, 25, 1), ReLU(), Linear(25, 2, 1), ReLU())\n",
    "train_model(model, train_input, train_target, mini_batch_size, 1e-6)\n",
    "nb_test_errors = compute_nb_errors(model, test_input, test_target, mini_batch_size)\n",
    "\n",
    "print(nb_test_errors)\n",
    "print('test error Net {:0.2f}%% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                      nb_test_errors, test_input.size(0)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
