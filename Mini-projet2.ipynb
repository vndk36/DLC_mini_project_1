{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import FloatTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NAME_INDEX = 0\n",
    "DATA_INDEX = 1\n",
    "GRAD_INDEX = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_disc_set(nb):\n",
    "    # Generates data and corresponding target\n",
    "    disk_center = FloatTensor(nb, 2).fill_(0.5)\n",
    "    input = FloatTensor(nb, 2).uniform_(0, 1)\n",
    "    \n",
    "    target = torch.norm(input.sub(disk_center), 2, 1) < math.sqrt(1/(2*math.pi))\n",
    "    target = torch.eye(2).index_select(0, target.long())   # transform the 1-D target into a matrix of two 1-D vectors\n",
    "                                                           # for the computation of the loss and its gradient. (see Part 2.1)\n",
    "    return input, target.float()                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter():\n",
    "    # class used to create all the parameters in the same way\n",
    "    \n",
    "    def __init__(self, name, tensor, gradient):\n",
    "        self.name = name       # name of the parameter\n",
    "        self.data = tensor     # parameter values\n",
    "        self.grad = gradient   # gradient of the parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module ( Parameter ) :\n",
    "    # base module that the following class inherits, see Part 3.1\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Parameter, self).__init__()\n",
    "        self.name = 'Base Module'  # name of the module\n",
    "        self.param = []            # contains all the parameters of the Module\n",
    "        \n",
    "    def forward (self, * input) :\n",
    "        # Computes the forward pass of the Module.\n",
    "        # Need to be implemented in future class if one wants to use it.\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward (self , * gradwrtoutput) :\n",
    "        # Computes the backward pass of the Module for the backpropagation of the loss.\n",
    "        # Need to be implemented in future class if one wants to use it.\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def init_parameters ( self ):\n",
    "        # Initialize the proper parameters for the Module.\n",
    "        # Need to be implemented in future class if one wants to use it.\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def add_parameter(self, parameter):\n",
    "        # Adds the input parameter to the already existing parameters of the Module\n",
    "        # Different implementation if the input parameter is:\n",
    "            # - an object of class 'Parameter'(when you initialize parameters in the current module)\n",
    "            # - a list of objects(when you add all the parameters of a module to another one)\n",
    "        if parameter.__class__.__name__ == 'Parameter':\n",
    "            self.param.append((parameter.name, parameter.data, parameter.grad))\n",
    "        elif parameter.__class__.__name__ == 'list':                        \n",
    "            if parameter != []:\n",
    "                self.param.append(parameter)\n",
    "                    \n",
    "    def zero_grad( self ):\n",
    "        # Reset the gradient of the parameters to 0\n",
    "        for i in range(len(self.param)):             # loop on the different Module initialized in the 'self' Module\n",
    "            for j in range(len(self.param[i])):      # loop on the parameters of each Module\n",
    "                self.param[i][j][GRAD_INDEX][:] = 0\n",
    "                    \n",
    "    def optimizer (self, lr = 1e-5):\n",
    "        # Optimization based on a Stochastic Gradient Descent\n",
    "        # updates the parameters in regard of their gradient and the input learning rate\n",
    "        for i in range(len(self.param)):             # loop on the different Module initialized in the 'self' Module\n",
    "            for j in range(len(self.param[i])):      # loop on the parameters of each Module\n",
    "                self.param[i][j][DATA_INDEX][:] -= lr * self.param[i][j][GRAD_INDEX][:]     # see formula (1)\n",
    "                \n",
    "    def parameters ( self ):\n",
    "        # returns the all parameters of the Module\n",
    "        return self.param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSEloss( Module ):\n",
    "    # Compute the Mean Square Error between the given input and target, see Part 3.2\n",
    "    # no parameters needed\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MSEloss, self).__init__()\n",
    "        self.name = 'MSEloss'\n",
    "        \n",
    "    def forward (self, input, target):             \n",
    "        return input.sub(target).pow(2).sum()       # see formula (2)\n",
    "    \n",
    "    def backward (self, input, target):\n",
    "        return 2*(input.sub(target))                # see formula (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU( Module ):\n",
    "    # Activation functions: Rectified Linear Unit on each element of the input, see Part 3.3.1\n",
    "    # no parameters needed\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "        self.name = 'ReLU'\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return input.clamp(min = 0)                 # see formula (4)\n",
    "    \n",
    "    def backward(self, input, dx):\n",
    "        input[input <= 0] = 0                       # see formula (5)\n",
    "        input[input > 0] = 1\n",
    "        return input * dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh( Module ):\n",
    "    # Activation functions: Hyperbolic tangent of each element of the input, see Part 3.3.2\n",
    "    # no parameters needed\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "        self.name = 'Tanh'\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return input.tanh()                      \n",
    "    \n",
    "    def backward(self, input, dx):\n",
    "        return (1 - input.tanh().pow(2)) * dx       # see formula (7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear( Module ):\n",
    "    # Linear transformation with certain weights and bias, see Part 3.4 \n",
    "    \n",
    "    Linear_counter = 0 # counter of the number of Linear module created in order to name properly the parameters\n",
    "    \n",
    "    def __init__(self, input_features, output_features, epsilon = 1e-1):\n",
    "        super(Linear, self).__init__()\n",
    "        self.name = 'Linear'\n",
    "        Linear.Linear_counter += 1        \n",
    "        self.init_parameters(input_features, output_features, epsilon)\n",
    "        \n",
    "    def init_parameters (self, input_features, output_features, epsilon):\n",
    "        # initializes the weight parameters with a normal distribution and set their gradient to 0\n",
    "        weigths_name = f'weights{self.Linear_counter}' # names the parameter accordingly to the number of linear object\n",
    "        self.weights = Parameter(weigths_name, torch.Tensor(input_features, output_features),\n",
    "                                 torch.Tensor(input_features, output_features))\n",
    "        self.weights.data.normal_(0, epsilon)\n",
    "        self.weights.grad.zero_()\n",
    "        \n",
    "        # initializes the bias parameters with a normal distribution and set their gradient to 0\n",
    "        bias_name = f'bias{self.Linear_counter}'       # names the parameter accordingly to the number of linear object\n",
    "        self.bias = Parameter(bias_name, torch.Tensor(output_features), torch.Tensor(output_features))  \n",
    "        self.bias.data.normal_(0, epsilon)\n",
    "        self.bias.grad.zero_()\n",
    "        \n",
    "        # adds the weight and bias parameters to this Module\n",
    "        self.add_parameter(self.weights)\n",
    "        self.add_parameter(self.bias)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = input.mm(self.weights.data)           # see formula (8)\n",
    "        output += self.bias.data\n",
    "        return output\n",
    "    \n",
    "    def backward(self, input, ds):\n",
    "        dx = ds.mm(self.weights.data.t())              # see formula (9)\n",
    "        dw = input.t().mm(ds)                          # see formula (10)\n",
    "        db = ds.t().mm(torch.ones(ds.size(0), 1))      # see formula (11)\n",
    "        \n",
    "        self.weights.grad.add_(dw)                     # updates the gradient of the parameters with the computed values\n",
    "        self.bias.grad.add_(db)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential( Module ):\n",
    "    # Sequential container of Modules object, see Part 3.5\n",
    "    # The order is given by the user when creating the object of class Sequential\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        super(Sequential, self).__init__()\n",
    "        Linear.Linear_counter = 0               # resets the number of Linear for each sequential object\n",
    "        self.module_nb = len(args)\n",
    "        self.fc = [None] * self.module_nb       # contains the modules object in order\n",
    "        self.x = [None] * (self.module_nb + 1)  # contains the input of the model \n",
    "                                                # and the intermediate results after the forward pass of each module \n",
    "        \n",
    "        for id, module in enumerate(args):      # fills the list containing the modules and adds their parameters to\n",
    "            self.fc[id] = module                # the sequential 'self' module\n",
    "            self.init_parameters(id)\n",
    "        \n",
    "    def init_parameters (self, id):\n",
    "        self.add_parameter(self.fc[id].parameters())\n",
    "        \n",
    "    def forward(self, input):                   # execute the forward pass of each module in the order given by the user\n",
    "        self.x[0] = input                       # fills the list with the intermediate output of the forward\n",
    "                                                # pass of each module \n",
    "        for i in range(1, self.module_nb + 1):  \n",
    "            self.x[i] = self.fc[i-1].forward(self.x[i-1])\n",
    "        return self.x[self.module_nb]\n",
    "    \n",
    "    def backward(self, dloss):                  # execute the backward pass of each module in the inverse order\n",
    "        dx = [None] * (self.module_nb + 1)      # compared to the forward pass\n",
    "        dx[self.module_nb] = dloss\n",
    "        \n",
    "        for i in range(1, self.module_nb + 1):\n",
    "            j = self.module_nb - i\n",
    "            dx[j] = self.fc[j].backward(self.x[j], dx[j+1])\n",
    "            \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, train_input, train_target, mini_batch_size = 100, lr = 1e-5, nb_epochs = 250):\n",
    "    # trains the given model 'nb_epochs' times and prints the loss for each iteration in the end\n",
    "    \n",
    "    sum_loss = FloatTensor(nb_epochs).zero_()                     # contains the loss after each iteration\n",
    "    \n",
    "    for e in range(0, nb_epochs):\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):  # divides the input data into batches\n",
    "            \n",
    "            output = model.forward(train_input.narrow(0, b, mini_batch_size))\n",
    "            \n",
    "            # Computation of the loss and its derivative to train the model\n",
    "            loss = criterion.forward(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            sum_loss[e] += loss\n",
    "            dloss = criterion.backward(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            \n",
    "            # reset the gradient before the backpropagation of the loss and gradient descent\n",
    "            # in order to update the parameters (learning phase)\n",
    "            model.zero_grad()\n",
    "            model.backward(dloss)\n",
    "            model.optimizer(lr)\n",
    "            \n",
    "        print('{:d}: loss = {:f}'.format(e, sum_loss[e]))\n",
    "    return sum_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, data_input, data_target, mini_batch_size = 100):\n",
    "    # computes the number of datapoint wrongly classified by the model\n",
    "    \n",
    "    total_nb_errors = 0\n",
    "    \n",
    "    for b in range(0, data_input.size(0), mini_batch_size):\n",
    "        \n",
    "        output = model.forward(data_input.narrow(0, b, mini_batch_size))   \n",
    "        _, predicted_classes = torch.max(output, 1)                        # the predicted class is the column of the output \n",
    "                                                                           # with the greater value \n",
    "        _, target_class = torch.max(data_target, 1)                        # same goes for the target (the column with 1)\n",
    "        \n",
    "        for k in range(0, mini_batch_size): \n",
    "            if (target_class[b + k] != predicted_classes[k]):                # compares the prediction with the target\n",
    "                total_nb_errors += 1                                       # increments when a point is wrongly classified\n",
    "\n",
    "    return total_nb_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dave/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:41: UserWarning: other is not broadcastable to self, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss = 621.475525\n",
      "1: loss = 497.918243\n",
      "2: loss = 495.190460\n",
      "3: loss = 492.469116\n",
      "4: loss = 488.791412\n",
      "5: loss = 484.010986\n",
      "6: loss = 477.414734\n",
      "7: loss = 469.463531\n",
      "8: loss = 459.672424\n",
      "9: loss = 447.087097\n",
      "10: loss = 431.646362\n",
      "11: loss = 413.885529\n",
      "12: loss = 393.912292\n",
      "13: loss = 372.307709\n",
      "14: loss = 349.829010\n",
      "15: loss = 327.016174\n",
      "16: loss = 304.491852\n",
      "17: loss = 282.255463\n",
      "18: loss = 260.530426\n",
      "19: loss = 239.997635\n",
      "20: loss = 221.777039\n",
      "21: loss = 207.138794\n",
      "22: loss = 196.653473\n",
      "23: loss = 189.694092\n",
      "24: loss = 185.289688\n",
      "25: loss = 182.416809\n",
      "26: loss = 180.477692\n",
      "27: loss = 178.972870\n",
      "28: loss = 177.574295\n",
      "29: loss = 176.445465\n",
      "30: loss = 175.340988\n",
      "31: loss = 174.416351\n",
      "32: loss = 173.178299\n",
      "33: loss = 172.002106\n",
      "34: loss = 170.499527\n",
      "35: loss = 168.957031\n",
      "36: loss = 167.361359\n",
      "37: loss = 165.821182\n",
      "38: loss = 164.313324\n",
      "39: loss = 163.004379\n",
      "40: loss = 161.752243\n",
      "41: loss = 160.771027\n",
      "42: loss = 159.804657\n",
      "43: loss = 158.834274\n",
      "44: loss = 158.113068\n",
      "45: loss = 157.378494\n",
      "46: loss = 156.688675\n",
      "47: loss = 155.936172\n",
      "48: loss = 155.345657\n",
      "49: loss = 154.603271\n",
      "50: loss = 153.983353\n",
      "51: loss = 153.305710\n",
      "52: loss = 152.666000\n",
      "53: loss = 152.171967\n",
      "54: loss = 151.726471\n",
      "55: loss = 151.147446\n",
      "56: loss = 150.437363\n",
      "57: loss = 149.623657\n",
      "58: loss = 148.472870\n",
      "59: loss = 147.025391\n",
      "60: loss = 145.607559\n",
      "61: loss = 144.158554\n",
      "62: loss = 142.596664\n",
      "63: loss = 141.400711\n",
      "64: loss = 139.846115\n",
      "65: loss = 138.414062\n",
      "66: loss = 137.173492\n",
      "67: loss = 136.145889\n",
      "68: loss = 134.790771\n",
      "69: loss = 133.642853\n",
      "70: loss = 132.810196\n",
      "71: loss = 131.970108\n",
      "72: loss = 131.352142\n",
      "73: loss = 130.765686\n",
      "74: loss = 130.240494\n",
      "75: loss = 129.669189\n",
      "76: loss = 129.081299\n",
      "77: loss = 128.524033\n",
      "78: loss = 128.134628\n",
      "79: loss = 127.609421\n",
      "80: loss = 127.139275\n",
      "81: loss = 126.669083\n",
      "82: loss = 126.094818\n",
      "83: loss = 125.584213\n",
      "84: loss = 125.080574\n",
      "85: loss = 124.359253\n",
      "86: loss = 123.611717\n",
      "87: loss = 123.030701\n",
      "88: loss = 122.509590\n",
      "89: loss = 121.959076\n",
      "90: loss = 121.427765\n",
      "91: loss = 120.942513\n",
      "92: loss = 120.450249\n",
      "93: loss = 119.966385\n",
      "94: loss = 119.379745\n",
      "95: loss = 118.966995\n",
      "96: loss = 118.381371\n",
      "97: loss = 117.855583\n",
      "98: loss = 117.286110\n",
      "99: loss = 116.902412\n",
      "100: loss = 116.433083\n",
      "101: loss = 115.961937\n",
      "102: loss = 115.558220\n",
      "103: loss = 115.113846\n",
      "104: loss = 114.698868\n",
      "105: loss = 114.234444\n",
      "106: loss = 113.859215\n",
      "107: loss = 113.475876\n",
      "108: loss = 113.125801\n",
      "109: loss = 112.752571\n",
      "110: loss = 112.434555\n",
      "111: loss = 112.100983\n",
      "112: loss = 111.731171\n",
      "113: loss = 111.405304\n",
      "114: loss = 111.100182\n",
      "115: loss = 110.795975\n",
      "116: loss = 110.588127\n",
      "117: loss = 110.349113\n",
      "118: loss = 109.994812\n",
      "119: loss = 109.687531\n",
      "120: loss = 109.371010\n",
      "121: loss = 109.139389\n",
      "122: loss = 108.892975\n",
      "123: loss = 108.577225\n",
      "124: loss = 108.254990\n",
      "125: loss = 107.916939\n",
      "126: loss = 107.658546\n",
      "127: loss = 107.429077\n",
      "128: loss = 106.871727\n",
      "129: loss = 106.475861\n",
      "130: loss = 106.168030\n",
      "131: loss = 105.794998\n",
      "132: loss = 105.423225\n",
      "133: loss = 105.054977\n",
      "134: loss = 104.732368\n",
      "135: loss = 104.399773\n",
      "136: loss = 103.996208\n",
      "137: loss = 103.550888\n",
      "138: loss = 103.158401\n",
      "139: loss = 102.887230\n",
      "140: loss = 102.637016\n",
      "141: loss = 102.245956\n",
      "142: loss = 101.824387\n",
      "143: loss = 101.580345\n",
      "144: loss = 101.327209\n",
      "145: loss = 101.091156\n",
      "146: loss = 100.802589\n",
      "147: loss = 100.582924\n",
      "148: loss = 100.358620\n",
      "149: loss = 100.090851\n",
      "150: loss = 99.828773\n",
      "151: loss = 99.593636\n",
      "152: loss = 99.380424\n",
      "153: loss = 99.188202\n",
      "154: loss = 99.008492\n",
      "155: loss = 98.854347\n",
      "156: loss = 98.757797\n",
      "157: loss = 98.601532\n",
      "158: loss = 98.498299\n",
      "159: loss = 98.435677\n",
      "160: loss = 98.229759\n",
      "161: loss = 98.201096\n",
      "162: loss = 98.035980\n",
      "163: loss = 97.992058\n",
      "164: loss = 97.866394\n",
      "165: loss = 97.777664\n",
      "166: loss = 97.659988\n",
      "167: loss = 97.580322\n",
      "168: loss = 97.482262\n",
      "169: loss = 97.325890\n",
      "170: loss = 97.358688\n",
      "171: loss = 97.265717\n",
      "172: loss = 97.183510\n",
      "173: loss = 97.096611\n",
      "174: loss = 96.985870\n",
      "175: loss = 96.983177\n",
      "176: loss = 96.865280\n",
      "177: loss = 96.797966\n",
      "178: loss = 96.679741\n",
      "179: loss = 96.629585\n",
      "180: loss = 96.581665\n",
      "181: loss = 96.483421\n",
      "182: loss = 96.382431\n",
      "183: loss = 96.305115\n",
      "184: loss = 96.265289\n",
      "185: loss = 96.200882\n",
      "186: loss = 96.114517\n",
      "187: loss = 96.049980\n",
      "188: loss = 95.990410\n",
      "189: loss = 95.881355\n",
      "190: loss = 95.805756\n",
      "191: loss = 95.689484\n",
      "192: loss = 95.708679\n",
      "193: loss = 95.689888\n",
      "194: loss = 95.493286\n",
      "195: loss = 95.513603\n",
      "196: loss = 95.362915\n",
      "197: loss = 95.406685\n",
      "198: loss = 95.168083\n",
      "199: loss = 95.122719\n",
      "200: loss = 95.136017\n",
      "201: loss = 94.975006\n",
      "202: loss = 94.965691\n",
      "203: loss = 94.872131\n",
      "204: loss = 94.779236\n",
      "205: loss = 94.722511\n",
      "206: loss = 94.630814\n",
      "207: loss = 94.562065\n",
      "208: loss = 94.523621\n",
      "209: loss = 94.399490\n",
      "210: loss = 94.434280\n",
      "211: loss = 94.259735\n",
      "212: loss = 94.286354\n",
      "213: loss = 94.091957\n",
      "214: loss = 94.055733\n",
      "215: loss = 93.949142\n",
      "216: loss = 93.871864\n",
      "217: loss = 93.863258\n",
      "218: loss = 93.781570\n",
      "219: loss = 93.730339\n",
      "220: loss = 93.661736\n",
      "221: loss = 93.538467\n",
      "222: loss = 93.522301\n",
      "223: loss = 93.484657\n",
      "224: loss = 93.399200\n",
      "225: loss = 93.352417\n",
      "226: loss = 93.218002\n",
      "227: loss = 93.223640\n",
      "228: loss = 93.176277\n",
      "229: loss = 93.048111\n",
      "230: loss = 93.061035\n",
      "231: loss = 92.971352\n",
      "232: loss = 92.927208\n",
      "233: loss = 92.823288\n",
      "234: loss = 92.875137\n",
      "235: loss = 92.682129\n",
      "236: loss = 92.688255\n",
      "237: loss = 92.632729\n",
      "238: loss = 92.545761\n",
      "239: loss = 92.440552\n",
      "240: loss = 92.477852\n",
      "241: loss = 92.317177\n",
      "242: loss = 92.318756\n",
      "243: loss = 92.200256\n",
      "244: loss = 92.195053\n",
      "245: loss = 92.073303\n",
      "246: loss = 92.019928\n",
      "247: loss = 92.018105\n",
      "248: loss = 91.892303\n",
      "249: loss = 91.912819\n",
      "\n",
      "train error: 3.40% (34/1000)\n",
      "test error: 3.30% 33/1000 \n",
      "\n",
      "Plot of the loss:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4a98b602e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Main Function\n",
    "\n",
    "# Generation of the train and test set\n",
    "train_input, train_target = generate_disc_set(1000)\n",
    "test_input, test_target = generate_disc_set(1000)\n",
    "\n",
    "mean, std = train_input.mean(), train_input.std()     # normailzation of the data\n",
    "train_input.sub_(mean).div_(std)\n",
    "test_input.sub_(mean).div_(std);\n",
    "\n",
    "# Training parameters\n",
    "mini_batch_size = 100\n",
    "nb_epochs = 250\n",
    "learning_rate = 1e-3\n",
    "standard_deviation = 0.1\n",
    "hidden_layer = 25\n",
    "\n",
    "# initilization of the model and criterion\n",
    "model = Sequential(Linear(2, hidden_layer, standard_deviation), Tanh(), \n",
    "                   Linear(hidden_layer, hidden_layer, standard_deviation), ReLU(), \n",
    "                   Linear(hidden_layer, 2, standard_deviation))\n",
    "criterion = MSEloss()\n",
    "\n",
    "# training of the model\n",
    "sum_loss = train_model(model, criterion, train_input, train_target, mini_batch_size, learning_rate, nb_epochs)\n",
    "\n",
    "# computing of the number of error\n",
    "nb_test_errors = compute_nb_errors(model, test_input, test_target, mini_batch_size)\n",
    "nb_train_errors = compute_nb_errors(model, train_input, train_target, mini_batch_size)\n",
    "\n",
    "# visualization\n",
    "print('\\ntrain error: {:0.2f}% ({:d}/{:d})'.format((100 * nb_train_errors) / train_input.size(0),   # number of errors on test\n",
    "                                                      nb_train_errors, train_input.size(0)))\n",
    "print('test error: {:0.2f}% {:d}/{:d} \\n'.format((100 * nb_test_errors) / test_input.size(0),      # number of errors on test\n",
    "                                                      nb_test_errors, test_input.size(0)))\n",
    "\n",
    "print('Plot of the loss:')\n",
    "plt.plot(range(nb_epochs), sum_loss.numpy())   # plots the loss at each iteration\n",
    "plt.xlabel('epochs n°')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
