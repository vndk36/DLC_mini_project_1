{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import FloatTensor\n",
    "\n",
    "NAME_INDEX = 0\n",
    "DATA_INDEX = 1\n",
    "GRAD_INDEX = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_disc_set(nb):\n",
    "    # Generates data and corresponding target\n",
    "    disk_center = FloatTensor(nb, 2).fill_(0.5)\n",
    "    input = FloatTensor(nb, 2).uniform_(0, 1)\n",
    "    target = torch.norm(input.sub(disk_center), 2, 1) < math.sqrt(1/(2*math.pi))\n",
    "    target = torch.eye(2).index_select(0, target.long())\n",
    "    return input, target.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter():\n",
    "    # class used to create all the parameters in the same way\n",
    "    \n",
    "    def __init__(self, name, tensor, gradient):\n",
    "        self.name = name       # name of the parameter\n",
    "        self.data = tensor     # parameter values\n",
    "        self.grad = gradient   # gradient of the parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module ( Parameter ) :\n",
    "    # base module that the following class inherits\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Parameter, self).__init__()\n",
    "        self.name = 'Base Module'  # name of the module\n",
    "        self.param = []            # contains all the parameters of the Module\n",
    "        \n",
    "    def forward (self, * input) :\n",
    "        # Computes the forward pass of the Module.\n",
    "        # Need to be implemented in future class if one wants to use it.\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward (self , * gradwrtoutput) :\n",
    "        # Computes the backward pass of the Module for the backpropagation of the loss.\n",
    "        # Need to be implemented in future class if one wants to use it.\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def init_parameters ( self ):\n",
    "        # Initialize the proper parameters for the Module.\n",
    "        # Need to be implemented in future class if one wants to use it.\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def add_parameter(self, parameter):\n",
    "        # Adds the input parameter to the already existing parameters of the Module.\n",
    "        # Different implementation if the input parameter is:\n",
    "            # - an object of class 'Parameter'(when you initialize parameters in the current module)\n",
    "            # - a list of objects(when you add all the parameters of a module to another one)\n",
    "        if parameter.__class__.__name__ == 'Parameter':\n",
    "            self.param.append((parameter.name, parameter.data, parameter.grad))\n",
    "        elif parameter.__class__.__name__ == 'list':                        \n",
    "            if parameter != []:\n",
    "                self.param.append(parameter)\n",
    "                    \n",
    "    def zero_grad( self ):\n",
    "        # Reset the gradient of the parameters to 0\n",
    "        for i in range(len(self.param)):            # loop on the different Module initialized in the 'self' Module\n",
    "            for j in range(len(self.param[i])):     # loop on the parameters of each Module\n",
    "                self.param[i][j][GRAD_INDEX][:] = 0\n",
    "                    \n",
    "    def optimizer (self, lr = 1e-5):\n",
    "        # Stochastic Gradient descent\n",
    "        # updates the parameters in regard of their gradient and the input learning rate\n",
    "        for i in range(len(self.param)):           # loop on the different Module initialized in the 'self' Module\n",
    "            for j in range(len(self.param[i])):    # loop on the parameters of each Module\n",
    "                self.param[i][j][DATA_INDEX][:] -= lr * self.param[i][j][GRAD_INDEX][:]\n",
    "                \n",
    "    def parameters ( self ):\n",
    "        # returns the all parameters of the Module\n",
    "        return self.param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSEloss( Module ):\n",
    "    # Compute the Mean Square Error between the given input and target\n",
    "    # no parameters needed\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MSEloss, self).__init__()\n",
    "        \n",
    "    def forward (self, input, target):\n",
    "        return input.sub(target).pow(2).sum() \n",
    "    \n",
    "    def backward (self, input, target):\n",
    "        return 2*(input.sub(target))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU( Module ):\n",
    "    # Activation functions: Rectified Linear Unit on each element of the input\n",
    "    # no parameters needed\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "        self.name = 'ReLU'\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return input.clamp(min = 0)\n",
    "    \n",
    "    def backward(self, input, dx):\n",
    "        input[input <= 0] = 0\n",
    "        input[input > 0] = 1\n",
    "        return input * dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module Tanh\n",
    "\n",
    "class Tanh( Module ):\n",
    "    # Activation functions: Hyperbolic tangent of each element of the input\n",
    "    # no parameters needed\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "        self.name = 'Tanh'\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return input.tanh()\n",
    "    \n",
    "    def backward(self, input, dx):\n",
    "        return 4 * (input.exp() + input.mul(-1).exp()).pow(-2) * dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear( Module ):\n",
    "    # Linear transformation with certain weights and bias\n",
    "    \n",
    "    Linear_counter = 0 # counter of the number of Linear module created in order to name properly the parameters\n",
    "    \n",
    "    def __init__(self, input_features, output_features, eps = 1e-2):\n",
    "        super(Linear, self).__init__()\n",
    "        self.name = 'Linear'\n",
    "        Linear.Linear_counter += 1        \n",
    "        self.init_parameters(input_features, output_features, eps)\n",
    "        \n",
    "    def init_parameters (self, input_features, output_features, eps):\n",
    "        # initializes the weight parameters with a normal distribution and set their gradient to 0\n",
    "        weigths_name = f'weights{self.Linear_counter}'\n",
    "        self.weights = Parameter(weigths_name, torch.Tensor(input_features, output_features),\n",
    "                                 torch.Tensor(input_features, output_features))\n",
    "        self.weights.data.normal_(0, eps)\n",
    "        self.weights.grad.zero_()\n",
    "        \n",
    "        # initializes the bias parameters with a normal distribution and set their gradient to 0\n",
    "        bias_name = f'bias{self.Linear_counter}'\n",
    "        self.bias = Parameter(bias_name, torch.Tensor(output_features), torch.Tensor(output_features))  \n",
    "        self.bias.data.normal_(0, eps)\n",
    "        self.bias.grad.zero_()\n",
    "        \n",
    "        # adds the weight and bias parameters to this Module\n",
    "        self.add_parameter(self.weights)\n",
    "        self.add_parameter(self.bias)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = input.mm(self.weights.data)\n",
    "        output += self.bias.data\n",
    "        return output\n",
    "    \n",
    "    def backward(self, input, ds):\n",
    "        dx = ds.mm(self.weights.data.t())\n",
    "        dw = input.t().mm(ds)\n",
    "        db = ds.t().mm(torch.ones(ds.size(0), 1))\n",
    "        \n",
    "        self.weights.grad.add_(dw) # updates the gradient of the parameters with the computed value\n",
    "        self.bias.grad.add_(db)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential( Module ):\n",
    "    # Sequential container of Modules object.\n",
    "    # The order is given by the user when creating the object of class Sequential\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        super(Sequential, self).__init__()\n",
    "        Linear.Linear_counter = 0               # resets the number of Linear for each sequential object\n",
    "        self.module_nb = len(args)\n",
    "        self.fc = [None] * self.module_nb       # contains the modules object in order\n",
    "        self.x = [None] * (self.module_nb + 1)  # contains the input of the model \n",
    "                                                # and the intermediate results after the forward pass of each module \n",
    "        \n",
    "        for id, module in enumerate(args):      # fills the list containing the modules and adds their parameters to\n",
    "            self.fc[id] = module                # the sequential 'self' module\n",
    "            self.init_parameters(id)\n",
    "        \n",
    "    def init_parameters (self, id):\n",
    "        self.add_parameter(self.fc[id].parameters())\n",
    "        \n",
    "    def forward(self, input):                   # execute the forward pass of each module in the order given by the user\n",
    "        self.x[0] = input                       # fills the list with the intermediate output of the forward\n",
    "                                                # pass of each module \n",
    "        for i in range(1, self.module_nb + 1):  \n",
    "            self.x[i] = self.fc[i-1].forward(self.x[i-1])\n",
    "        return self.x[self.module_nb]\n",
    "    \n",
    "    def backward(self, dloss):                  # execute the backward pass of each module in the inverse order\n",
    "        dx = [None] * (self.module_nb + 1)      # compared to the forward pass\n",
    "        dx[self.module_nb] = dloss\n",
    "        \n",
    "        for i in range(1, self.module_nb + 1):\n",
    "            j = self.module_nb - i\n",
    "            dx[j] = self.fc[j].backward(self.x[j], dx[j+1])\n",
    "            \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net 4 layers\n",
    "\n",
    "Linear.Linear_counter = 0\n",
    "\n",
    "class Net4( Module ):\n",
    "    def __init__(self, hidden_layer):\n",
    "        super(Net4, self).__init__()\n",
    "        self.fc1 = Linear(2, 2*hidden_layer)\n",
    "        self.fc2 = ReLU()\n",
    "        self.fc3 = Linear(2*hidden_layer, hidden_layer)\n",
    "        self.fc4 = Linear(hidden_layer, 1)\n",
    "        self.init_parameters()\n",
    "        \n",
    "    def init_parameters ( self ):\n",
    "        self.add_parameter( self.fc1.parameters() )\n",
    "        self.add_parameter( self.fc2.parameters() )\n",
    "        self.add_parameter( self.fc3.parameters() )\n",
    "        self.add_parameter( self.fc4.parameters() )\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x0 = x\n",
    "        x = self.fc1.forward( x )\n",
    "        self.s1 = x\n",
    "        x = self.fc2.forward( x )\n",
    "        self.x1 = x\n",
    "        x = self.fc3.forward( x )\n",
    "        self.s2 = x\n",
    "        x = self.fc2.forward( x )\n",
    "        self.x2 = x\n",
    "        x = self.fc4.forward( x )\n",
    "        self.s3 = x\n",
    "        x = self.fc2.forward( x )\n",
    "        self.x3 = x\n",
    "        return x\n",
    "    \n",
    "    def backward(self, dloss):\n",
    "        dx3 = dloss\n",
    "        ds3 = self.fc2.backward(self.s3) * dx3\n",
    "        dx2, dw4, db4 = self.fc4.backward( ds3, self.x2 )\n",
    "        self.fc4.weights.grad.add_(dw4)\n",
    "        self.fc4.bias.grad.add_(db4)\n",
    "        ds2 = self.fc2.backward(self.s2) * dx2\n",
    "        dx1, dw3, db3 = self.fc3.backward( ds2 , self.x1 )\n",
    "        self.fc3.weights.grad.add_(dw3)\n",
    "        self.fc3.bias.grad.add_(db3)\n",
    "        \n",
    "        ds1 = self.fc2.backward(self.s1) * dx1\n",
    "        \n",
    "        dx0, dw1, db1 = self.fc1.backward( ds1 , self.x0 )\n",
    "\n",
    "        self.fc1.weights.grad.add_(dw1)\n",
    "        self.fc1.bias.grad.add_(db1)\n",
    "        \n",
    "        return dx0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, train_input, train_target, mini_batch_size = 100, lr = 1e-1, nb_epochs = 250):\n",
    "    # trains the given model 'nb_epochs' times and prints the loss for each iteration in the end\n",
    "    \n",
    "    sum_loss = FloatTensor(nb_epochs).zero_()                     # contains the loss after each iteration\n",
    "    \n",
    "    for e in range(0, nb_epochs):\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):  # divides the input data into batches\n",
    "            # input go through the model\n",
    "            output = model.forward(train_input.narrow(0, b, mini_batch_size))\n",
    "            \n",
    "            # loss and dloss computation to train the model and visualize it\n",
    "            loss = criterion.forward(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            sum_loss[e] += loss\n",
    "            dloss = criterion.backward(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            \n",
    "            # reset the gradient before the backpropagation of the loss by gradient descent\n",
    "            # in order to update the parameters (learning phase)\n",
    "            model.zero_grad()\n",
    "            model.backward(dloss)\n",
    "            model.optimizer(lr)\n",
    "            \n",
    "        nb_test_errors = compute_nb_errors(model, test_input, test_target, mini_batch_size)\n",
    "        nb_train_errors = compute_nb_errors(model, train_input, train_target, mini_batch_size)\n",
    "        print('test error Net {:0.2f}%% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                      nb_test_errors, test_input.size(0)))\n",
    "        print('train error Net {:0.2f}%% {:d}/{:d}'.format((100 * nb_train_errors) / train_input.size(0),\n",
    "                                                      nb_train_errors, train_input.size(0)))\n",
    "            \n",
    "        print(e, sum_loss[e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, data_input, data_target, mini_batch_size = 100):\n",
    "    # computes the number of datapoint wrongly classified by the model\n",
    "    \n",
    "    total_nb_errors = 0\n",
    "    \n",
    "    for b in range(0, data_input.size(0), mini_batch_size):\n",
    "        \n",
    "        output = model.forward(data_input.narrow(0, b, mini_batch_size))   # the predicted class is the column of the output with the greater value\n",
    "        #print(output)\n",
    "        _, predicted_classes = torch.max(output, 1)                        # the predicted class \n",
    "        #print(predicted_classes)\n",
    "        _, target_class = torch.max(data_target, 1)\n",
    "        \n",
    "        for k in range(0, mini_batch_size): \n",
    "            if target_class[b + k] != predicted_classes[k]:\n",
    "                total_nb_errors += 1\n",
    "\n",
    "    return total_nb_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dave/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:41: UserWarning: other is not broadcastable to self, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error Net 57.00%% 570/1000\n",
      "train error Net 57.70%% 577/1000\n",
      "0 80237.4296875\n",
      "test error Net 58.40%% 584/1000\n",
      "train error Net 60.60%% 606/1000\n",
      "1 13055.748046875\n",
      "test error Net 54.70%% 547/1000\n",
      "train error Net 55.90%% 559/1000\n",
      "2 6189.01904296875\n",
      "test error Net 51.30%% 513/1000\n",
      "train error Net 55.10%% 551/1000\n",
      "3 4117.88427734375\n",
      "test error Net 49.40%% 494/1000\n",
      "train error Net 52.80%% 528/1000\n",
      "4 3299.1142578125\n",
      "test error Net 47.00%% 470/1000\n",
      "train error Net 50.00%% 500/1000\n",
      "5 2834.868408203125\n",
      "test error Net 44.70%% 447/1000\n",
      "train error Net 47.00%% 470/1000\n",
      "6 2515.505126953125\n",
      "test error Net 41.00%% 410/1000\n",
      "train error Net 43.80%% 438/1000\n",
      "7 2274.06494140625\n",
      "test error Net 38.30%% 383/1000\n",
      "train error Net 41.50%% 415/1000\n",
      "8 2084.276611328125\n",
      "test error Net 34.90%% 349/1000\n",
      "train error Net 37.80%% 378/1000\n",
      "9 1930.0452880859375\n",
      "test error Net 32.10%% 321/1000\n",
      "train error Net 35.00%% 350/1000\n",
      "10 1802.9315185546875\n",
      "test error Net 30.00%% 300/1000\n",
      "train error Net 32.80%% 328/1000\n",
      "11 1694.0135498046875\n",
      "test error Net 29.50%% 295/1000\n",
      "train error Net 31.10%% 311/1000\n",
      "12 1598.47509765625\n",
      "test error Net 28.10%% 281/1000\n",
      "train error Net 30.00%% 300/1000\n",
      "13 1513.8792724609375\n",
      "test error Net 27.10%% 271/1000\n",
      "train error Net 28.50%% 285/1000\n",
      "14 1439.070556640625\n",
      "test error Net 25.40%% 254/1000\n",
      "train error Net 27.00%% 270/1000\n",
      "15 1373.8162841796875\n",
      "test error Net 24.70%% 247/1000\n",
      "train error Net 25.90%% 259/1000\n",
      "16 1315.873291015625\n",
      "test error Net 24.40%% 244/1000\n",
      "train error Net 25.10%% 251/1000\n",
      "17 1263.328125\n",
      "test error Net 23.50%% 235/1000\n",
      "train error Net 24.70%% 247/1000\n",
      "18 1215.644775390625\n",
      "test error Net 23.20%% 232/1000\n",
      "train error Net 24.20%% 242/1000\n",
      "19 1172.265869140625\n",
      "test error Net 23.10%% 231/1000\n",
      "train error Net 23.70%% 237/1000\n",
      "20 1132.3975830078125\n",
      "test error Net 22.50%% 225/1000\n",
      "train error Net 23.50%% 235/1000\n",
      "21 1096.228759765625\n",
      "test error Net 22.30%% 223/1000\n",
      "train error Net 23.60%% 236/1000\n",
      "22 1063.07177734375\n",
      "test error Net 21.60%% 216/1000\n",
      "train error Net 22.80%% 228/1000\n",
      "23 1032.591552734375\n",
      "test error Net 21.30%% 213/1000\n",
      "train error Net 22.40%% 224/1000\n",
      "24 1004.2525634765625\n",
      "test error Net 21.50%% 215/1000\n",
      "train error Net 21.90%% 219/1000\n",
      "25 977.6656494140625\n",
      "test error Net 20.60%% 206/1000\n",
      "train error Net 21.30%% 213/1000\n",
      "26 952.42138671875\n",
      "test error Net 20.20%% 202/1000\n",
      "train error Net 20.60%% 206/1000\n",
      "27 929.0733032226562\n",
      "test error Net 19.50%% 195/1000\n",
      "train error Net 20.60%% 206/1000\n",
      "28 906.901611328125\n",
      "test error Net 19.70%% 197/1000\n",
      "train error Net 20.20%% 202/1000\n",
      "29 885.9861450195312\n",
      "test error Net 19.80%% 198/1000\n",
      "train error Net 19.90%% 199/1000\n",
      "30 866.0994873046875\n",
      "test error Net 19.60%% 196/1000\n",
      "train error Net 19.80%% 198/1000\n",
      "31 847.30419921875\n",
      "test error Net 19.50%% 195/1000\n",
      "train error Net 19.70%% 197/1000\n",
      "32 829.3338012695312\n",
      "test error Net 19.40%% 194/1000\n",
      "train error Net 19.50%% 195/1000\n",
      "33 812.6008911132812\n",
      "test error Net 19.30%% 193/1000\n",
      "train error Net 19.40%% 194/1000\n",
      "34 796.9744873046875\n",
      "test error Net 19.40%% 194/1000\n",
      "train error Net 19.10%% 191/1000\n",
      "35 782.1571044921875\n",
      "test error Net 19.40%% 194/1000\n",
      "train error Net 18.90%% 189/1000\n",
      "36 768.0540161132812\n",
      "test error Net 19.20%% 192/1000\n",
      "train error Net 18.50%% 185/1000\n",
      "37 754.7536010742188\n",
      "test error Net 19.10%% 191/1000\n",
      "train error Net 18.10%% 181/1000\n",
      "38 741.9531860351562\n",
      "test error Net 19.00%% 190/1000\n",
      "train error Net 18.20%% 182/1000\n",
      "39 729.83544921875\n",
      "test error Net 18.90%% 189/1000\n",
      "train error Net 18.10%% 181/1000\n",
      "40 718.4201049804688\n",
      "test error Net 18.50%% 185/1000\n",
      "train error Net 17.70%% 177/1000\n",
      "41 707.4905395507812\n",
      "test error Net 18.30%% 183/1000\n",
      "train error Net 17.40%% 174/1000\n",
      "42 697.0956420898438\n",
      "test error Net 18.50%% 185/1000\n",
      "train error Net 17.40%% 174/1000\n",
      "43 686.9497680664062\n",
      "test error Net 18.60%% 186/1000\n",
      "train error Net 17.40%% 174/1000\n",
      "44 677.0369873046875\n",
      "test error Net 18.50%% 185/1000\n",
      "train error Net 17.50%% 175/1000\n",
      "45 667.6864013671875\n",
      "test error Net 18.00%% 180/1000\n",
      "train error Net 17.60%% 176/1000\n",
      "46 658.6403198242188\n",
      "test error Net 17.80%% 178/1000\n",
      "train error Net 17.40%% 174/1000\n",
      "47 649.7676391601562\n",
      "test error Net 17.70%% 177/1000\n",
      "train error Net 17.30%% 173/1000\n",
      "48 641.3450927734375\n",
      "test error Net 17.70%% 177/1000\n",
      "train error Net 17.30%% 173/1000\n",
      "49 633.228759765625\n",
      "test error Net 17.50%% 175/1000\n",
      "train error Net 16.90%% 169/1000\n",
      "50 625.4547119140625\n",
      "test error Net 17.40%% 174/1000\n",
      "train error Net 16.90%% 169/1000\n",
      "51 617.9431762695312\n",
      "test error Net 17.10%% 171/1000\n",
      "train error Net 16.80%% 168/1000\n",
      "52 610.79248046875\n",
      "test error Net 17.00%% 170/1000\n",
      "train error Net 16.70%% 167/1000\n",
      "53 603.971923828125\n",
      "test error Net 16.90%% 169/1000\n",
      "train error Net 16.60%% 166/1000\n",
      "54 597.447509765625\n",
      "test error Net 16.70%% 167/1000\n",
      "train error Net 16.50%% 165/1000\n",
      "55 591.1451416015625\n",
      "test error Net 16.60%% 166/1000\n",
      "train error Net 16.30%% 163/1000\n",
      "56 585.0624389648438\n",
      "test error Net 16.60%% 166/1000\n",
      "train error Net 16.20%% 162/1000\n",
      "57 578.9380493164062\n",
      "test error Net 16.70%% 167/1000\n",
      "train error Net 16.10%% 161/1000\n",
      "58 572.9630737304688\n",
      "test error Net 16.60%% 166/1000\n",
      "train error Net 15.80%% 158/1000\n",
      "59 567.1678466796875\n",
      "test error Net 16.50%% 165/1000\n",
      "train error Net 15.60%% 156/1000\n",
      "60 561.62158203125\n",
      "test error Net 16.30%% 163/1000\n",
      "train error Net 15.40%% 154/1000\n",
      "61 556.32470703125\n",
      "test error Net 16.30%% 163/1000\n",
      "train error Net 15.40%% 154/1000\n",
      "62 551.2503662109375\n",
      "test error Net 16.10%% 161/1000\n",
      "train error Net 15.30%% 153/1000\n",
      "63 546.37109375\n",
      "test error Net 15.90%% 159/1000\n",
      "train error Net 15.20%% 152/1000\n",
      "64 541.701416015625\n",
      "test error Net 15.90%% 159/1000\n",
      "train error Net 15.30%% 153/1000\n",
      "65 537.2266845703125\n",
      "test error Net 15.80%% 158/1000\n",
      "train error Net 15.20%% 152/1000\n",
      "66 532.7655639648438\n",
      "test error Net 15.60%% 156/1000\n",
      "train error Net 15.10%% 151/1000\n",
      "67 528.3370361328125\n",
      "test error Net 15.40%% 154/1000\n",
      "train error Net 14.90%% 149/1000\n",
      "68 524.012939453125\n",
      "test error Net 15.20%% 152/1000\n",
      "train error Net 14.80%% 148/1000\n",
      "69 519.7806396484375\n",
      "test error Net 15.10%% 151/1000\n",
      "train error Net 14.70%% 147/1000\n",
      "70 515.6319580078125\n",
      "test error Net 15.00%% 150/1000\n",
      "train error Net 14.80%% 148/1000\n",
      "71 511.5165100097656\n",
      "test error Net 14.90%% 149/1000\n",
      "train error Net 14.80%% 148/1000\n",
      "72 507.51519775390625\n",
      "test error Net 14.70%% 147/1000\n",
      "train error Net 14.80%% 148/1000\n",
      "73 503.5981140136719\n",
      "test error Net 14.70%% 147/1000\n",
      "train error Net 14.50%% 145/1000\n",
      "74 499.5171813964844\n",
      "test error Net 14.60%% 146/1000\n",
      "train error Net 14.40%% 144/1000\n",
      "75 495.5664367675781\n",
      "test error Net 14.70%% 147/1000\n",
      "train error Net 14.40%% 144/1000\n",
      "76 491.72039794921875\n",
      "test error Net 14.70%% 147/1000\n",
      "train error Net 14.20%% 142/1000\n",
      "77 487.9134826660156\n",
      "test error Net 14.70%% 147/1000\n",
      "train error Net 14.10%% 141/1000\n",
      "78 484.2339172363281\n",
      "test error Net 14.80%% 148/1000\n",
      "train error Net 14.00%% 140/1000\n",
      "79 480.636962890625\n",
      "test error Net 14.80%% 148/1000\n",
      "train error Net 13.70%% 137/1000\n",
      "80 477.1133117675781\n",
      "test error Net 14.90%% 149/1000\n",
      "train error Net 13.80%% 138/1000\n",
      "81 473.6311950683594\n",
      "test error Net 14.90%% 149/1000\n",
      "train error Net 13.50%% 135/1000\n",
      "82 470.17083740234375\n",
      "test error Net 14.90%% 149/1000\n",
      "train error Net 13.60%% 136/1000\n",
      "83 466.7364196777344\n",
      "test error Net 15.00%% 150/1000\n",
      "train error Net 13.40%% 134/1000\n",
      "84 463.38116455078125\n",
      "test error Net 14.80%% 148/1000\n",
      "train error Net 13.20%% 132/1000\n",
      "85 460.0779113769531\n",
      "test error Net 14.90%% 149/1000\n",
      "train error Net 13.20%% 132/1000\n",
      "86 456.8406066894531\n",
      "test error Net 14.60%% 146/1000\n",
      "train error Net 13.10%% 131/1000\n",
      "87 453.6349792480469\n",
      "test error Net 14.30%% 143/1000\n",
      "train error Net 13.20%% 132/1000\n",
      "88 450.3480224609375\n",
      "test error Net 14.30%% 143/1000\n",
      "train error Net 13.10%% 131/1000\n",
      "89 447.1241760253906\n",
      "test error Net 14.30%% 143/1000\n",
      "train error Net 12.60%% 126/1000\n",
      "90 443.96490478515625\n",
      "test error Net 14.00%% 140/1000\n",
      "train error Net 12.60%% 126/1000\n",
      "91 440.90435791015625\n",
      "test error Net 13.90%% 139/1000\n",
      "train error Net 12.40%% 124/1000\n",
      "92 437.894287109375\n",
      "test error Net 13.80%% 138/1000\n",
      "train error Net 12.50%% 125/1000\n",
      "93 434.94366455078125\n",
      "test error Net 13.80%% 138/1000\n",
      "train error Net 12.40%% 124/1000\n",
      "94 432.07403564453125\n",
      "test error Net 13.80%% 138/1000\n",
      "train error Net 12.40%% 124/1000\n",
      "95 429.2589111328125\n",
      "test error Net 13.50%% 135/1000\n",
      "train error Net 12.40%% 124/1000\n",
      "96 426.4989013671875\n",
      "test error Net 13.00%% 130/1000\n",
      "train error Net 12.20%% 122/1000\n",
      "97 423.75067138671875\n",
      "test error Net 12.80%% 128/1000\n",
      "train error Net 12.20%% 122/1000\n",
      "98 421.07611083984375\n",
      "test error Net 12.70%% 127/1000\n",
      "train error Net 12.10%% 121/1000\n",
      "99 418.4580993652344\n",
      "test error Net 12.60%% 126/1000\n",
      "train error Net 12.10%% 121/1000\n",
      "100 415.8946228027344\n",
      "test error Net 12.50%% 125/1000\n",
      "train error Net 12.00%% 120/1000\n",
      "101 413.3188781738281\n",
      "test error Net 12.40%% 124/1000\n",
      "train error Net 12.00%% 120/1000\n",
      "102 410.66888427734375\n",
      "test error Net 12.10%% 121/1000\n",
      "train error Net 11.60%% 116/1000\n",
      "103 408.0660095214844\n",
      "test error Net 12.00%% 120/1000\n",
      "train error Net 11.40%% 114/1000\n",
      "104 405.48876953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error Net 12.00%% 120/1000\n",
      "train error Net 11.30%% 113/1000\n",
      "105 402.9253234863281\n",
      "test error Net 12.00%% 120/1000\n",
      "train error Net 11.20%% 112/1000\n",
      "106 400.4118347167969\n",
      "test error Net 12.00%% 120/1000\n",
      "train error Net 11.20%% 112/1000\n",
      "107 397.7730712890625\n",
      "test error Net 11.80%% 118/1000\n",
      "train error Net 11.20%% 112/1000\n",
      "108 395.216064453125\n",
      "test error Net 11.80%% 118/1000\n",
      "train error Net 11.20%% 112/1000\n",
      "109 392.7259826660156\n",
      "test error Net 11.70%% 117/1000\n",
      "train error Net 11.20%% 112/1000\n",
      "110 390.20025634765625\n",
      "test error Net 11.60%% 116/1000\n",
      "train error Net 11.10%% 111/1000\n",
      "111 387.7444763183594\n",
      "test error Net 11.40%% 114/1000\n",
      "train error Net 11.10%% 111/1000\n",
      "112 385.33978271484375\n",
      "test error Net 11.40%% 114/1000\n",
      "train error Net 10.90%% 109/1000\n",
      "113 382.99163818359375\n",
      "test error Net 11.30%% 113/1000\n",
      "train error Net 10.80%% 108/1000\n",
      "114 380.6382751464844\n",
      "test error Net 11.30%% 113/1000\n",
      "train error Net 10.70%% 107/1000\n",
      "115 378.26318359375\n",
      "test error Net 11.20%% 112/1000\n",
      "train error Net 10.40%% 104/1000\n",
      "116 376.00030517578125\n",
      "test error Net 11.10%% 111/1000\n",
      "train error Net 10.30%% 103/1000\n",
      "117 373.80474853515625\n",
      "test error Net 11.10%% 111/1000\n",
      "train error Net 10.30%% 103/1000\n",
      "118 371.5690002441406\n",
      "test error Net 11.00%% 110/1000\n",
      "train error Net 10.30%% 103/1000\n",
      "119 369.41033935546875\n",
      "test error Net 11.00%% 110/1000\n",
      "train error Net 10.30%% 103/1000\n",
      "120 367.3090515136719\n",
      "test error Net 10.80%% 108/1000\n",
      "train error Net 10.20%% 102/1000\n",
      "121 365.2535400390625\n",
      "test error Net 10.70%% 107/1000\n",
      "train error Net 10.10%% 101/1000\n",
      "122 363.2230529785156\n",
      "test error Net 10.60%% 106/1000\n",
      "train error Net 10.20%% 102/1000\n",
      "123 361.2301025390625\n",
      "test error Net 10.40%% 104/1000\n",
      "train error Net 10.20%% 102/1000\n",
      "124 359.2679138183594\n",
      "test error Net 10.40%% 104/1000\n",
      "train error Net 10.20%% 102/1000\n",
      "125 357.3529052734375\n",
      "test error Net 10.40%% 104/1000\n",
      "train error Net 10.20%% 102/1000\n",
      "126 355.4977111816406\n",
      "test error Net 10.40%% 104/1000\n",
      "train error Net 10.10%% 101/1000\n",
      "127 353.6877136230469\n",
      "test error Net 10.50%% 105/1000\n",
      "train error Net 10.00%% 100/1000\n",
      "128 351.89013671875\n",
      "test error Net 10.50%% 105/1000\n",
      "train error Net 9.90%% 99/1000\n",
      "129 350.1170959472656\n",
      "test error Net 10.40%% 104/1000\n",
      "train error Net 9.70%% 97/1000\n",
      "130 348.3865051269531\n",
      "test error Net 10.30%% 103/1000\n",
      "train error Net 9.60%% 96/1000\n",
      "131 346.6741027832031\n",
      "test error Net 10.30%% 103/1000\n",
      "train error Net 9.60%% 96/1000\n",
      "132 344.9591064453125\n",
      "test error Net 10.20%% 102/1000\n",
      "train error Net 9.60%% 96/1000\n",
      "133 343.2591552734375\n",
      "test error Net 10.20%% 102/1000\n",
      "train error Net 9.30%% 93/1000\n",
      "134 341.5689392089844\n",
      "test error Net 10.30%% 103/1000\n",
      "train error Net 9.30%% 93/1000\n",
      "135 339.9110412597656\n",
      "test error Net 10.30%% 103/1000\n",
      "train error Net 9.30%% 93/1000\n",
      "136 338.27337646484375\n",
      "test error Net 10.30%% 103/1000\n",
      "train error Net 9.30%% 93/1000\n",
      "137 336.67315673828125\n",
      "test error Net 10.30%% 103/1000\n",
      "train error Net 9.20%% 92/1000\n",
      "138 335.1280517578125\n",
      "test error Net 10.40%% 104/1000\n",
      "train error Net 9.00%% 90/1000\n",
      "139 333.59552001953125\n",
      "test error Net 10.50%% 105/1000\n",
      "train error Net 8.90%% 89/1000\n",
      "140 332.1030578613281\n",
      "test error Net 10.40%% 104/1000\n",
      "train error Net 8.90%% 89/1000\n",
      "141 330.6772766113281\n",
      "test error Net 10.40%% 104/1000\n",
      "train error Net 8.90%% 89/1000\n",
      "142 329.27972412109375\n",
      "test error Net 10.40%% 104/1000\n",
      "train error Net 8.90%% 89/1000\n",
      "143 327.89227294921875\n",
      "test error Net 10.20%% 102/1000\n",
      "train error Net 8.70%% 87/1000\n",
      "144 326.5401916503906\n",
      "test error Net 10.20%% 102/1000\n",
      "train error Net 8.70%% 87/1000\n",
      "145 325.2107238769531\n",
      "test error Net 10.10%% 101/1000\n",
      "train error Net 8.70%% 87/1000\n",
      "146 323.92596435546875\n",
      "test error Net 10.10%% 101/1000\n",
      "train error Net 8.70%% 87/1000\n",
      "147 322.65069580078125\n",
      "test error Net 10.00%% 100/1000\n",
      "train error Net 8.70%% 87/1000\n",
      "148 321.3858642578125\n",
      "test error Net 10.00%% 100/1000\n",
      "train error Net 8.70%% 87/1000\n",
      "149 320.10833740234375\n",
      "test error Net 10.00%% 100/1000\n",
      "train error Net 8.70%% 87/1000\n",
      "150 318.87432861328125\n",
      "test error Net 9.90%% 99/1000\n",
      "train error Net 8.70%% 87/1000\n",
      "151 317.6720886230469\n",
      "test error Net 9.80%% 98/1000\n",
      "train error Net 8.60%% 86/1000\n",
      "152 316.4668884277344\n",
      "test error Net 9.80%% 98/1000\n",
      "train error Net 8.60%% 86/1000\n",
      "153 315.2498474121094\n",
      "test error Net 9.80%% 98/1000\n",
      "train error Net 8.50%% 85/1000\n",
      "154 314.0503234863281\n",
      "test error Net 9.70%% 97/1000\n",
      "train error Net 8.40%% 84/1000\n",
      "155 312.8427734375\n",
      "test error Net 9.70%% 97/1000\n",
      "train error Net 8.40%% 84/1000\n",
      "156 311.65313720703125\n",
      "test error Net 9.70%% 97/1000\n",
      "train error Net 8.30%% 83/1000\n",
      "157 310.46844482421875\n",
      "test error Net 9.70%% 97/1000\n",
      "train error Net 8.30%% 83/1000\n",
      "158 309.3385009765625\n",
      "test error Net 9.70%% 97/1000\n",
      "train error Net 8.30%% 83/1000\n",
      "159 308.2216491699219\n",
      "test error Net 9.70%% 97/1000\n",
      "train error Net 8.30%% 83/1000\n",
      "160 307.1120910644531\n",
      "test error Net 9.70%% 97/1000\n",
      "train error Net 8.20%% 82/1000\n",
      "161 305.9989318847656\n",
      "test error Net 9.70%% 97/1000\n",
      "train error Net 8.10%% 81/1000\n",
      "162 304.9052734375\n",
      "test error Net 9.70%% 97/1000\n",
      "train error Net 8.10%% 81/1000\n",
      "163 303.8299255371094\n",
      "test error Net 9.70%% 97/1000\n",
      "train error Net 8.00%% 80/1000\n",
      "164 302.73419189453125\n",
      "test error Net 9.70%% 97/1000\n",
      "train error Net 7.90%% 79/1000\n",
      "165 301.686767578125\n",
      "test error Net 9.60%% 96/1000\n",
      "train error Net 7.90%% 79/1000\n",
      "166 300.64117431640625\n",
      "test error Net 9.60%% 96/1000\n",
      "train error Net 7.90%% 79/1000\n",
      "167 299.6256408691406\n",
      "test error Net 9.50%% 95/1000\n",
      "train error Net 7.90%% 79/1000\n",
      "168 298.637939453125\n",
      "test error Net 9.50%% 95/1000\n",
      "train error Net 7.90%% 79/1000\n",
      "169 297.66705322265625\n",
      "test error Net 9.50%% 95/1000\n",
      "train error Net 7.90%% 79/1000\n",
      "170 296.72088623046875\n",
      "test error Net 9.50%% 95/1000\n",
      "train error Net 8.00%% 80/1000\n",
      "171 295.8022766113281\n",
      "test error Net 9.40%% 94/1000\n",
      "train error Net 7.80%% 78/1000\n",
      "172 294.90264892578125\n",
      "test error Net 9.40%% 94/1000\n",
      "train error Net 7.70%% 77/1000\n",
      "173 294.0159912109375\n",
      "test error Net 9.40%% 94/1000\n",
      "train error Net 7.60%% 76/1000\n",
      "174 293.16766357421875\n",
      "test error Net 9.40%% 94/1000\n",
      "train error Net 7.60%% 76/1000\n",
      "175 292.3090515136719\n",
      "test error Net 9.30%% 93/1000\n",
      "train error Net 7.60%% 76/1000\n",
      "176 291.46112060546875\n",
      "test error Net 9.30%% 93/1000\n",
      "train error Net 7.60%% 76/1000\n",
      "177 290.61614990234375\n",
      "test error Net 9.30%% 93/1000\n",
      "train error Net 7.60%% 76/1000\n",
      "178 289.7637634277344\n",
      "test error Net 9.30%% 93/1000\n",
      "train error Net 7.60%% 76/1000\n",
      "179 288.9246520996094\n",
      "test error Net 9.30%% 93/1000\n",
      "train error Net 7.60%% 76/1000\n",
      "180 288.093017578125\n",
      "test error Net 9.10%% 91/1000\n",
      "train error Net 7.60%% 76/1000\n",
      "181 287.2623291015625\n",
      "test error Net 9.10%% 91/1000\n",
      "train error Net 7.60%% 76/1000\n",
      "182 286.3389892578125\n",
      "test error Net 9.10%% 91/1000\n",
      "train error Net 7.40%% 74/1000\n",
      "183 285.32843017578125\n",
      "test error Net 9.10%% 91/1000\n",
      "train error Net 7.30%% 73/1000\n",
      "184 284.3567810058594\n",
      "test error Net 9.10%% 91/1000\n",
      "train error Net 7.30%% 73/1000\n",
      "185 283.40740966796875\n",
      "test error Net 9.10%% 91/1000\n",
      "train error Net 7.20%% 72/1000\n",
      "186 282.477294921875\n",
      "test error Net 9.00%% 90/1000\n",
      "train error Net 7.20%% 72/1000\n",
      "187 281.5842590332031\n",
      "test error Net 9.00%% 90/1000\n",
      "train error Net 7.20%% 72/1000\n",
      "188 280.7466125488281\n",
      "test error Net 9.00%% 90/1000\n",
      "train error Net 6.90%% 69/1000\n",
      "189 279.941162109375\n",
      "test error Net 9.10%% 91/1000\n",
      "train error Net 6.80%% 68/1000\n",
      "190 279.1499938964844\n",
      "test error Net 9.10%% 91/1000\n",
      "train error Net 6.80%% 68/1000\n",
      "191 278.37713623046875\n",
      "test error Net 9.00%% 90/1000\n",
      "train error Net 6.80%% 68/1000\n",
      "192 277.62054443359375\n",
      "test error Net 9.00%% 90/1000\n",
      "train error Net 6.80%% 68/1000\n",
      "193 276.8770446777344\n",
      "test error Net 9.00%% 90/1000\n",
      "train error Net 6.80%% 68/1000\n",
      "194 276.1405029296875\n",
      "test error Net 9.00%% 90/1000\n",
      "train error Net 6.80%% 68/1000\n",
      "195 275.4231262207031\n",
      "test error Net 9.00%% 90/1000\n",
      "train error Net 6.80%% 68/1000\n",
      "196 274.73284912109375\n",
      "test error Net 9.00%% 90/1000\n",
      "train error Net 6.80%% 68/1000\n",
      "197 274.0452575683594\n",
      "test error Net 9.10%% 91/1000\n",
      "train error Net 6.80%% 68/1000\n",
      "198 273.34979248046875\n",
      "test error Net 9.10%% 91/1000\n",
      "train error Net 6.80%% 68/1000\n",
      "199 272.65118408203125\n",
      "test error Net 9.10%% 91/1000\n",
      "train error Net 6.80%% 68/1000\n",
      "200 271.96893310546875\n",
      "test error Net 9.10%% 91/1000\n",
      "train error Net 6.80%% 68/1000\n",
      "201 271.2989196777344\n",
      "test error Net 9.00%% 90/1000\n",
      "train error Net 6.80%% 68/1000\n",
      "202 270.6493835449219\n",
      "test error Net 9.00%% 90/1000\n",
      "train error Net 6.80%% 68/1000\n",
      "203 270.0047912597656\n",
      "test error Net 9.00%% 90/1000\n",
      "train error Net 6.80%% 68/1000\n",
      "204 269.3506774902344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error Net 9.00%% 90/1000\n",
      "train error Net 6.70%% 67/1000\n",
      "205 268.7044982910156\n",
      "test error Net 9.00%% 90/1000\n",
      "train error Net 6.60%% 66/1000\n",
      "206 268.0622863769531\n",
      "test error Net 9.00%% 90/1000\n",
      "train error Net 6.60%% 66/1000\n",
      "207 267.42523193359375\n",
      "test error Net 9.00%% 90/1000\n",
      "train error Net 6.60%% 66/1000\n",
      "208 266.7952880859375\n",
      "test error Net 9.00%% 90/1000\n",
      "train error Net 6.50%% 65/1000\n",
      "209 266.1618347167969\n",
      "test error Net 9.00%% 90/1000\n",
      "train error Net 6.50%% 65/1000\n",
      "210 265.5461730957031\n",
      "test error Net 9.00%% 90/1000\n",
      "train error Net 6.50%% 65/1000\n",
      "211 264.9281311035156\n",
      "test error Net 9.00%% 90/1000\n",
      "train error Net 6.50%% 65/1000\n",
      "212 264.3302917480469\n",
      "test error Net 8.90%% 89/1000\n",
      "train error Net 6.50%% 65/1000\n",
      "213 263.73614501953125\n",
      "test error Net 8.90%% 89/1000\n",
      "train error Net 6.50%% 65/1000\n",
      "214 263.15435791015625\n",
      "test error Net 8.90%% 89/1000\n",
      "train error Net 6.50%% 65/1000\n",
      "215 262.6260070800781\n",
      "test error Net 8.80%% 88/1000\n",
      "train error Net 6.50%% 65/1000\n",
      "216 262.0260925292969\n",
      "test error Net 8.90%% 89/1000\n",
      "train error Net 6.40%% 64/1000\n",
      "217 261.4420166015625\n",
      "test error Net 8.80%% 88/1000\n",
      "train error Net 6.30%% 63/1000\n",
      "218 260.8590087890625\n",
      "test error Net 8.80%% 88/1000\n",
      "train error Net 6.30%% 63/1000\n",
      "219 260.290771484375\n",
      "test error Net 8.80%% 88/1000\n",
      "train error Net 6.30%% 63/1000\n",
      "220 259.7348937988281\n",
      "test error Net 8.80%% 88/1000\n",
      "train error Net 6.30%% 63/1000\n",
      "221 259.1941223144531\n",
      "test error Net 8.80%% 88/1000\n",
      "train error Net 6.30%% 63/1000\n",
      "222 258.6533203125\n",
      "test error Net 8.80%% 88/1000\n",
      "train error Net 6.30%% 63/1000\n",
      "223 258.1242370605469\n",
      "test error Net 8.80%% 88/1000\n",
      "train error Net 6.20%% 62/1000\n",
      "224 257.59173583984375\n",
      "test error Net 8.80%% 88/1000\n",
      "train error Net 6.20%% 62/1000\n",
      "225 257.0655822753906\n",
      "test error Net 8.80%% 88/1000\n",
      "train error Net 6.30%% 63/1000\n",
      "226 256.55322265625\n",
      "test error Net 8.80%% 88/1000\n",
      "train error Net 6.30%% 63/1000\n",
      "227 256.0343933105469\n",
      "test error Net 8.80%% 88/1000\n",
      "train error Net 6.10%% 61/1000\n",
      "228 255.52587890625\n",
      "test error Net 8.80%% 88/1000\n",
      "train error Net 6.10%% 61/1000\n",
      "229 255.02850341796875\n",
      "test error Net 8.80%% 88/1000\n",
      "train error Net 6.10%% 61/1000\n",
      "230 254.5347442626953\n",
      "test error Net 8.80%% 88/1000\n",
      "train error Net 6.10%% 61/1000\n",
      "231 254.04946899414062\n",
      "test error Net 8.80%% 88/1000\n",
      "train error Net 6.10%% 61/1000\n",
      "232 253.57864379882812\n",
      "test error Net 8.80%% 88/1000\n",
      "train error Net 6.10%% 61/1000\n",
      "233 253.07997131347656\n",
      "test error Net 8.80%% 88/1000\n",
      "train error Net 6.10%% 61/1000\n",
      "234 252.5845947265625\n",
      "test error Net 8.80%% 88/1000\n",
      "train error Net 6.00%% 60/1000\n",
      "235 252.10475158691406\n",
      "test error Net 8.80%% 88/1000\n",
      "train error Net 6.00%% 60/1000\n",
      "236 251.63572692871094\n",
      "test error Net 8.80%% 88/1000\n",
      "train error Net 5.90%% 59/1000\n",
      "237 251.1761016845703\n",
      "test error Net 8.70%% 87/1000\n",
      "train error Net 5.90%% 59/1000\n",
      "238 250.72335815429688\n",
      "test error Net 8.70%% 87/1000\n",
      "train error Net 5.90%% 59/1000\n",
      "239 250.27476501464844\n",
      "test error Net 8.70%% 87/1000\n",
      "train error Net 5.90%% 59/1000\n",
      "240 249.83706665039062\n",
      "test error Net 8.70%% 87/1000\n",
      "train error Net 5.90%% 59/1000\n",
      "241 249.40359497070312\n",
      "test error Net 8.70%% 87/1000\n",
      "train error Net 5.90%% 59/1000\n",
      "242 248.9803924560547\n",
      "test error Net 8.70%% 87/1000\n",
      "train error Net 5.90%% 59/1000\n",
      "243 248.56529235839844\n",
      "test error Net 8.60%% 86/1000\n",
      "train error Net 5.90%% 59/1000\n",
      "244 248.15121459960938\n",
      "test error Net 8.60%% 86/1000\n",
      "train error Net 5.90%% 59/1000\n",
      "245 247.7390899658203\n",
      "test error Net 8.60%% 86/1000\n",
      "train error Net 5.90%% 59/1000\n",
      "246 247.32981872558594\n",
      "test error Net 8.60%% 86/1000\n",
      "train error Net 5.80%% 58/1000\n",
      "247 246.93434143066406\n",
      "test error Net 8.60%% 86/1000\n",
      "train error Net 5.80%% 58/1000\n",
      "248 246.5361785888672\n",
      "test error Net 8.60%% 86/1000\n",
      "train error Net 5.80%% 58/1000\n",
      "249 246.14749145507812\n",
      "86\n",
      "test error Net 8.60%% 86/1000\n",
      "train error Net 5.80%% 58/1000\n"
     ]
    }
   ],
   "source": [
    "# Main Function\n",
    "\n",
    "# Computing of the train and test set\n",
    "train_input, train_target = generate_disc_set(1000)\n",
    "test_input, test_target = generate_disc_set(1000)\n",
    "mean, std = train_input.mean(), train_input.std()\n",
    "train_input.sub_(mean).div_(std)\n",
    "test_input.sub_(mean).div_(std);\n",
    "\n",
    "# Training parameters\n",
    "mini_batch_size = 100\n",
    "nb_epochs = 250\n",
    "\n",
    "# initilization of the model and criterion\n",
    "model = Sequential(Linear(2, 25, 1), ReLU(), Linear(25, 25, 1), ReLU(), Linear(25, 2, 1))\n",
    "criterion = MSEloss()\n",
    "\n",
    "# training of the model\n",
    "train_model(model, criterion, train_input, train_target, mini_batch_size, 1e-5, nb_epochs)\n",
    "\n",
    "# computing of the number of error\n",
    "nb_test_errors = compute_nb_errors(model, test_input, test_target, mini_batch_size)\n",
    "nb_train_errors = compute_nb_errors(model, train_input, train_target, mini_batch_size)\n",
    "\n",
    "# visualization\n",
    "print(nb_test_errors)\n",
    "print('test error Net {:0.2f}%% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                      nb_test_errors, test_input.size(0)))\n",
    "print('train error Net {:0.2f}%% {:d}/{:d}'.format((100 * nb_train_errors) / train_input.size(0),\n",
    "                                                      nb_train_errors, train_input.size(0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of type Variable[torch.LongTensor] but found type Variable[torch.ByteTensor] for argument #1 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-00870e50bf70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m print('std {:f} train_error {:.02f}% test_error {:.02f}%'.format(\n",
      "\u001b[0;32m<ipython-input-83-00870e50bf70>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_input, train_target)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         return F.cross_entropy(input, target, self.weight, self.size_average,\n\u001b[0;32m--> 679\u001b[0;31m                                self.ignore_index, self.reduce)\n\u001b[0m\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \"\"\"\n\u001b[0;32m-> 1161\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of type Variable[torch.LongTensor] but found type Variable[torch.ByteTensor] for argument #1 'target'"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def generate_disc_set(nb):\n",
    "    input = Tensor(nb, 2).uniform_(0, 1)\n",
    "    target = torch.norm(input, 2, 1) < math.sqrt(1/(2*math.pi))\n",
    "    return input, target\n",
    "\n",
    "train_input, train_target = generate_disc_set(1000)\n",
    "test_input, test_target = generate_disc_set(1000)\n",
    "\n",
    "mean, std = train_input.mean(), train_input.std()\n",
    "\n",
    "train_input.sub_(mean).div_(std)\n",
    "test_input.sub_(mean).div_(std)\n",
    "\n",
    "train_input, train_target = Variable(train_input), Variable(train_target)\n",
    "test_input, test_target = Variable(test_input), Variable(test_target)\n",
    "\n",
    "mini_batch_size = 100\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def train_model(model, train_input, train_target):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr = 1e-1)\n",
    "    nb_epochs = 250\n",
    "\n",
    "    for e in range(0, nb_epochs):\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def compute_nb_errors(model, data_input, data_target):\n",
    "\n",
    "    nb_data_errors = 0\n",
    "\n",
    "    for b in range(0, data_input.size(0), mini_batch_size):\n",
    "        output = model(data_input.narrow(0, b, mini_batch_size))\n",
    "        print(output)\n",
    "        _, predicted_classes = torch.max(output.data, 1)\n",
    "        for k in range(0, mini_batch_size):\n",
    "            if data_target.data[b + k] != predicted_classes[k]:\n",
    "                nb_data_errors = nb_data_errors + 1\n",
    "\n",
    "    return nb_data_errors\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def create_shallow_model():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(2, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 2)\n",
    "    )\n",
    "\n",
    "######################################################################\n",
    "\n",
    "model = create_shallow_model()\n",
    "for p in model.parameters(): p.data.normal_(0, 1e-1)\n",
    "\n",
    "train_model(model, train_input, train_target)\n",
    "\n",
    "print('std {:f} train_error {:.02f}% test_error {:.02f}%'.format(\n",
    "        std,\n",
    "        compute_nb_errors(model, train_input, train_target) / train_input.size(0) * 100,\n",
    "        compute_nb_errors(model, test_input, test_target) / test_input.size(0) * 100\n",
    "    )\n",
    "    )\n",
    "\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
