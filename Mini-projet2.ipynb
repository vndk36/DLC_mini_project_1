{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import FloatTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NAME_INDEX = 0\n",
    "DATA_INDEX = 1\n",
    "GRAD_INDEX = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_disc_set(nb):\n",
    "    # Generates data and corresponding target\n",
    "    disk_center = FloatTensor(nb, 2).fill_(0.5)\n",
    "    input = FloatTensor(nb, 2).uniform_(0, 1)\n",
    "    \n",
    "    target = torch.norm(input.sub(disk_center), 2, 1) < math.sqrt(1/(2*math.pi))\n",
    "    target = torch.eye(2).index_select(0, target.long())   # transform the 1-D target into a matrix of two 1-D vectors\n",
    "                                                           # for the computation of the loss and its gradient. (see Part 2.1)\n",
    "    return input, target.float()                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter():\n",
    "    # class used to create all the parameters in the same way\n",
    "    \n",
    "    def __init__(self, name, tensor, gradient):\n",
    "        self.name = name       # name of the parameter\n",
    "        self.data = tensor     # parameter values\n",
    "        self.grad = gradient   # gradient of the parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module ( Parameter ) :\n",
    "    # base module that the following class inherits, see Part 3.1\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Parameter, self).__init__()\n",
    "        self.name = 'Base Module'  # name of the module\n",
    "        self.param = []            # contains all the parameters of the Module\n",
    "        \n",
    "    def forward (self, * input) :\n",
    "        # Computes the forward pass of the Module.\n",
    "        # Need to be implemented in future class if one wants to use it.\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward (self , * gradwrtoutput) :\n",
    "        # Computes the backward pass of the Module for the backpropagation of the loss.\n",
    "        # Need to be implemented in future class if one wants to use it.\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def init_parameters ( self ):\n",
    "        # Initialize the proper parameters for the Module.\n",
    "        # Need to be implemented in future class if one wants to use it.\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def add_parameter(self, parameter):\n",
    "        # Adds the input parameter to the already existing parameters of the Module.\n",
    "        # Different implementation if the input parameter is:\n",
    "            # - an object of class 'Parameter'(when you initialize parameters in the current module)\n",
    "            # - a list of objects(when you add all the parameters of a module to another one)\n",
    "        if parameter.__class__.__name__ == 'Parameter':\n",
    "            self.param.append((parameter.name, parameter.data, parameter.grad))\n",
    "        elif parameter.__class__.__name__ == 'list':                        \n",
    "            if parameter != []:\n",
    "                self.param.append(parameter)\n",
    "                    \n",
    "    def zero_grad( self ):\n",
    "        # Reset the gradient of the parameters to 0\n",
    "        for i in range(len(self.param)):             # loop on the different Module initialized in the 'self' Module\n",
    "            for j in range(len(self.param[i])):      # loop on the parameters of each Module\n",
    "                self.param[i][j][GRAD_INDEX][:] = 0\n",
    "                    \n",
    "    def optimizer (self, lr = 1e-5):\n",
    "        # Optimization based on a Stochastic Gradient Descent\n",
    "        # updates the parameters in regard of their gradient and the input learning rate\n",
    "        for i in range(len(self.param)):             # loop on the different Module initialized in the 'self' Module\n",
    "            for j in range(len(self.param[i])):      # loop on the parameters of each Module\n",
    "                self.param[i][j][DATA_INDEX][:] -= lr * self.param[i][j][GRAD_INDEX][:]     # see formula ()\n",
    "                \n",
    "    def parameters ( self ):\n",
    "        # returns the all parameters of the Module\n",
    "        return self.param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSEloss( Module ):\n",
    "    # Compute the Mean Square Error between the given input and target\n",
    "    # no parameters needed\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MSEloss, self).__init__()\n",
    "        self.name = 'MSEloss'\n",
    "        \n",
    "    def forward (self, input, target):             \n",
    "        return input.sub(target).pow(2).sum()       # see formula (1)\n",
    "    \n",
    "    def backward (self, input, target):\n",
    "        return 2*(input.sub(target))                # see formula (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU( Module ):\n",
    "    # Activation functions: Rectified Linear Unit on each element of the input\n",
    "    # no parameters needed\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "        self.name = 'ReLU'\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return input.clamp(min = 0)                 # see formula (3)\n",
    "    \n",
    "    def backward(self, input, dx):\n",
    "        input[input <= 0] = 0                       # see formula (4)\n",
    "        input[input > 0] = 1\n",
    "        return input * dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module Tanh\n",
    "\n",
    "class Tanh( Module ):\n",
    "    # Activation functions: Hyperbolic tangent of each element of the input\n",
    "    # no parameters needed\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "        self.name = 'Tanh'\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return input.tanh()                      \n",
    "    \n",
    "    def backward(self, input, dx):\n",
    "        return (1 - input.tanh().pow(2)) * dx       # see formula (6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear( Module ):\n",
    "    # Linear transformation with certain weights and bias\n",
    "    \n",
    "    Linear_counter = 0 # counter of the number of Linear module created in order to name properly the parameters\n",
    "    \n",
    "    def __init__(self, input_features, output_features, epsilon = 1e-1):\n",
    "        super(Linear, self).__init__()\n",
    "        self.name = 'Linear'\n",
    "        Linear.Linear_counter += 1        \n",
    "        self.init_parameters(input_features, output_features, epsilon)\n",
    "        \n",
    "    def init_parameters (self, input_features, output_features, epsilon):\n",
    "        # initializes the weight parameters with a normal distribution and set their gradient to 0\n",
    "        weigths_name = f'weights{self.Linear_counter}' # names the parameter accordingly to the number of linear object\n",
    "        self.weights = Parameter(weigths_name, torch.Tensor(input_features, output_features),\n",
    "                                 torch.Tensor(input_features, output_features))\n",
    "        self.weights.data.normal_(0, epsilon)\n",
    "        self.weights.grad.zero_()\n",
    "        \n",
    "        # initializes the bias parameters with a normal distribution and set their gradient to 0\n",
    "        bias_name = f'bias{self.Linear_counter}'       # names the parameter accordingly to the number of linear object\n",
    "        self.bias = Parameter(bias_name, torch.Tensor(output_features), torch.Tensor(output_features))  \n",
    "        self.bias.data.normal_(0, epsilon)\n",
    "        self.bias.grad.zero_()\n",
    "        \n",
    "        # adds the weight and bias parameters to this Module\n",
    "        self.add_parameter(self.weights)\n",
    "        self.add_parameter(self.bias)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = input.mm(self.weights.data)           # see formula ()\n",
    "        output += self.bias.data\n",
    "        return output\n",
    "    \n",
    "    def backward(self, input, ds):\n",
    "        dx = ds.mm(self.weights.data.t())              # see formula ()\n",
    "        dw = input.t().mm(ds)\n",
    "        db = ds.t().mm(torch.ones(ds.size(0), 1))\n",
    "        \n",
    "        self.weights.grad.add_(dw)                     # updates the gradient of the parameters with the computed values\n",
    "        self.bias.grad.add_(db)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential( Module ):\n",
    "    # Sequential container of Modules object.\n",
    "    # The order is given by the user when creating the object of class Sequential\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        super(Sequential, self).__init__()\n",
    "        Linear.Linear_counter = 0               # resets the number of Linear for each sequential object\n",
    "        self.module_nb = len(args)\n",
    "        self.fc = [None] * self.module_nb       # contains the modules object in order\n",
    "        self.x = [None] * (self.module_nb + 1)  # contains the input of the model \n",
    "                                                # and the intermediate results after the forward pass of each module \n",
    "        \n",
    "        for id, module in enumerate(args):      # fills the list containing the modules and adds their parameters to\n",
    "            self.fc[id] = module                # the sequential 'self' module\n",
    "            self.init_parameters(id)\n",
    "        \n",
    "    def init_parameters (self, id):\n",
    "        self.add_parameter(self.fc[id].parameters())\n",
    "        \n",
    "    def forward(self, input):                   # execute the forward pass of each module in the order given by the user\n",
    "        self.x[0] = input                       # fills the list with the intermediate output of the forward\n",
    "                                                # pass of each module \n",
    "        for i in range(1, self.module_nb + 1):  \n",
    "            self.x[i] = self.fc[i-1].forward(self.x[i-1])\n",
    "        return self.x[self.module_nb]\n",
    "    \n",
    "    def backward(self, dloss):                  # execute the backward pass of each module in the inverse order\n",
    "        dx = [None] * (self.module_nb + 1)      # compared to the forward pass\n",
    "        dx[self.module_nb] = dloss\n",
    "        \n",
    "        for i in range(1, self.module_nb + 1):\n",
    "            j = self.module_nb - i\n",
    "            dx[j] = self.fc[j].backward(self.x[j], dx[j+1])\n",
    "            \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, train_input, train_target, mini_batch_size = 100, lr = 1e-5, nb_epochs = 250):\n",
    "    # trains the given model 'nb_epochs' times and prints the loss for each iteration in the end\n",
    "    \n",
    "    sum_loss = FloatTensor(nb_epochs).zero_()                     # contains the loss after each iteration\n",
    "    \n",
    "    for e in range(0, nb_epochs):\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):  # divides the input data into batches\n",
    "            output = model.forward(train_input.narrow(0, b, mini_batch_size))\n",
    "            \n",
    "            # loss and dloss computation to train the model\n",
    "            loss = criterion.forward(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            sum_loss[e] += loss\n",
    "            dloss = criterion.backward(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            \n",
    "            # reset the gradient before the backpropagation of the loss and gradient descent\n",
    "            # in order to update the parameters (learning phase)\n",
    "            model.zero_grad()\n",
    "            model.backward(dloss)\n",
    "            model.optimizer(lr)\n",
    "            \n",
    "        print('{:d}: loss = {:f}'.format(e, sum_loss[e]))\n",
    "    return sum_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, data_input, data_target, mini_batch_size = 100):\n",
    "    # computes the number of datapoint wrongly classified by the model\n",
    "    \n",
    "    total_nb_errors = 0\n",
    "    \n",
    "    for b in range(0, data_input.size(0), mini_batch_size):\n",
    "        \n",
    "        output = model.forward(data_input.narrow(0, b, mini_batch_size))   \n",
    "        _, predicted_classes = torch.max(output, 1)                        # the predicted class is the column of the output \n",
    "                                                                           # with the greater value \n",
    "        _, target_class = torch.max(data_target, 1)                        # same goes for the target (the column with 1)\n",
    "        \n",
    "        for k in range(0, mini_batch_size): \n",
    "            if (target_class[b + k] != predicted_classes[k]):                # compares the prediction with the target\n",
    "                total_nb_errors += 1                                       # increments when a point is wrongly classified\n",
    "\n",
    "    return total_nb_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dave/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:41: UserWarning: other is not broadcastable to self, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss = 658.276917\n",
      "1: loss = 489.277283\n",
      "2: loss = 480.486603\n",
      "3: loss = 470.522583\n",
      "4: loss = 458.673218\n",
      "5: loss = 444.270996\n",
      "6: loss = 426.377228\n",
      "7: loss = 404.384460\n",
      "8: loss = 378.354919\n",
      "9: loss = 349.094604\n",
      "10: loss = 318.565399\n",
      "11: loss = 288.787323\n",
      "12: loss = 261.464478\n",
      "13: loss = 237.820419\n",
      "14: loss = 218.507843\n",
      "15: loss = 203.413773\n",
      "16: loss = 192.386383\n",
      "17: loss = 184.738144\n",
      "18: loss = 179.800629\n",
      "19: loss = 176.551147\n",
      "20: loss = 174.277588\n",
      "21: loss = 172.506592\n",
      "22: loss = 170.962448\n",
      "23: loss = 169.617111\n",
      "24: loss = 168.271561\n",
      "25: loss = 167.031357\n",
      "26: loss = 165.930542\n",
      "27: loss = 164.793747\n",
      "28: loss = 163.375610\n",
      "29: loss = 162.255127\n",
      "30: loss = 161.154800\n",
      "31: loss = 160.025391\n",
      "32: loss = 158.859070\n",
      "33: loss = 157.567520\n",
      "34: loss = 156.443665\n",
      "35: loss = 155.498581\n",
      "36: loss = 154.601440\n",
      "37: loss = 153.714478\n",
      "38: loss = 152.921494\n",
      "39: loss = 152.192871\n",
      "40: loss = 151.383713\n",
      "41: loss = 150.385376\n",
      "42: loss = 149.391373\n",
      "43: loss = 148.423798\n",
      "44: loss = 147.667007\n",
      "45: loss = 147.034149\n",
      "46: loss = 146.309082\n",
      "47: loss = 145.497589\n",
      "48: loss = 144.816742\n",
      "49: loss = 144.109222\n",
      "50: loss = 143.424576\n",
      "51: loss = 142.894211\n",
      "52: loss = 141.981018\n",
      "53: loss = 141.187897\n",
      "54: loss = 140.458099\n",
      "55: loss = 139.773193\n",
      "56: loss = 139.143463\n",
      "57: loss = 138.560013\n",
      "58: loss = 137.943329\n",
      "59: loss = 137.483658\n",
      "60: loss = 137.021561\n",
      "61: loss = 136.555023\n",
      "62: loss = 136.141068\n",
      "63: loss = 135.762161\n",
      "64: loss = 135.290680\n",
      "65: loss = 134.866364\n",
      "66: loss = 134.333817\n",
      "67: loss = 133.914139\n",
      "68: loss = 133.502411\n",
      "69: loss = 133.117294\n",
      "70: loss = 132.527527\n",
      "71: loss = 132.109650\n",
      "72: loss = 131.753143\n",
      "73: loss = 131.238647\n",
      "74: loss = 130.537399\n",
      "75: loss = 130.008484\n",
      "76: loss = 129.560333\n",
      "77: loss = 129.174072\n",
      "78: loss = 128.767029\n",
      "79: loss = 128.371384\n",
      "80: loss = 127.951202\n",
      "81: loss = 127.484123\n",
      "82: loss = 127.071007\n",
      "83: loss = 126.632744\n",
      "84: loss = 125.916321\n",
      "85: loss = 125.463303\n",
      "86: loss = 124.913193\n",
      "87: loss = 124.504745\n",
      "88: loss = 124.090622\n",
      "89: loss = 123.710915\n",
      "90: loss = 123.418854\n",
      "91: loss = 122.994194\n",
      "92: loss = 122.647095\n",
      "93: loss = 122.362656\n",
      "94: loss = 122.039742\n",
      "95: loss = 121.723747\n",
      "96: loss = 121.497292\n",
      "97: loss = 121.058678\n",
      "98: loss = 120.735641\n",
      "99: loss = 120.241165\n",
      "100: loss = 119.802963\n",
      "101: loss = 119.368530\n",
      "102: loss = 118.865631\n",
      "103: loss = 118.336060\n",
      "104: loss = 117.865433\n",
      "105: loss = 117.239937\n",
      "106: loss = 116.807365\n",
      "107: loss = 116.250710\n",
      "108: loss = 115.823334\n",
      "109: loss = 115.453087\n",
      "110: loss = 115.053001\n",
      "111: loss = 114.731079\n",
      "112: loss = 114.344032\n",
      "113: loss = 113.950760\n",
      "114: loss = 113.589973\n",
      "115: loss = 113.286339\n",
      "116: loss = 112.943115\n",
      "117: loss = 112.611084\n",
      "118: loss = 112.271111\n",
      "119: loss = 111.974190\n",
      "120: loss = 111.599312\n",
      "121: loss = 111.272827\n",
      "122: loss = 110.956268\n",
      "123: loss = 110.613846\n",
      "124: loss = 110.265923\n",
      "125: loss = 110.014549\n",
      "126: loss = 109.691231\n",
      "127: loss = 109.293030\n",
      "128: loss = 108.963112\n",
      "129: loss = 108.613205\n",
      "130: loss = 108.313034\n",
      "131: loss = 108.044846\n",
      "132: loss = 107.742516\n",
      "133: loss = 107.396667\n",
      "134: loss = 107.011932\n",
      "135: loss = 106.700737\n",
      "136: loss = 106.393501\n",
      "137: loss = 105.992538\n",
      "138: loss = 105.747612\n",
      "139: loss = 105.448288\n",
      "140: loss = 105.232216\n",
      "141: loss = 104.958565\n",
      "142: loss = 104.685699\n",
      "143: loss = 104.413025\n",
      "144: loss = 104.161263\n",
      "145: loss = 103.885605\n",
      "146: loss = 103.642319\n",
      "147: loss = 103.437813\n",
      "148: loss = 103.188286\n",
      "149: loss = 102.963585\n",
      "150: loss = 102.741463\n",
      "151: loss = 102.454155\n",
      "152: loss = 102.257217\n",
      "153: loss = 102.119514\n",
      "154: loss = 101.830887\n",
      "155: loss = 101.585609\n",
      "156: loss = 101.316093\n",
      "157: loss = 101.103539\n",
      "158: loss = 100.915398\n",
      "159: loss = 100.756973\n",
      "160: loss = 100.567421\n",
      "161: loss = 100.343460\n",
      "162: loss = 100.135376\n",
      "163: loss = 100.045021\n",
      "164: loss = 99.873810\n",
      "165: loss = 99.648895\n",
      "166: loss = 99.466095\n",
      "167: loss = 99.253326\n",
      "168: loss = 99.087578\n",
      "169: loss = 98.823792\n",
      "170: loss = 98.747307\n",
      "171: loss = 98.616562\n",
      "172: loss = 98.464989\n",
      "173: loss = 98.379898\n",
      "174: loss = 98.191940\n",
      "175: loss = 98.091194\n",
      "176: loss = 97.938110\n",
      "177: loss = 97.816780\n",
      "178: loss = 97.652489\n",
      "179: loss = 97.551743\n",
      "180: loss = 97.399704\n",
      "181: loss = 97.322403\n",
      "182: loss = 97.183716\n",
      "183: loss = 97.099968\n",
      "184: loss = 97.013176\n",
      "185: loss = 96.839355\n",
      "186: loss = 96.757965\n",
      "187: loss = 96.707443\n",
      "188: loss = 96.606369\n",
      "189: loss = 96.518188\n",
      "190: loss = 96.460083\n",
      "191: loss = 96.341156\n",
      "192: loss = 96.279404\n",
      "193: loss = 96.189270\n",
      "194: loss = 96.113647\n",
      "195: loss = 96.068352\n",
      "196: loss = 96.013557\n",
      "197: loss = 95.912277\n",
      "198: loss = 95.899811\n",
      "199: loss = 95.825943\n",
      "200: loss = 95.741119\n",
      "201: loss = 95.676758\n",
      "202: loss = 95.602913\n",
      "203: loss = 95.551270\n",
      "204: loss = 95.487694\n",
      "205: loss = 95.458755\n",
      "206: loss = 95.339684\n",
      "207: loss = 95.308617\n",
      "208: loss = 95.254601\n",
      "209: loss = 95.200485\n",
      "210: loss = 95.137306\n",
      "211: loss = 95.061218\n",
      "212: loss = 95.009392\n",
      "213: loss = 94.943695\n",
      "214: loss = 94.940697\n",
      "215: loss = 94.867447\n",
      "216: loss = 94.866379\n",
      "217: loss = 94.796860\n",
      "218: loss = 94.740456\n",
      "219: loss = 94.692139\n",
      "220: loss = 94.671196\n",
      "221: loss = 94.630417\n",
      "222: loss = 94.634796\n",
      "223: loss = 94.559052\n",
      "224: loss = 94.522163\n",
      "225: loss = 94.498909\n",
      "226: loss = 94.442543\n",
      "227: loss = 94.443092\n",
      "228: loss = 94.355072\n",
      "229: loss = 94.375786\n",
      "230: loss = 94.318909\n",
      "231: loss = 94.295044\n",
      "232: loss = 94.228638\n",
      "233: loss = 94.173454\n",
      "234: loss = 94.199448\n",
      "235: loss = 94.107941\n",
      "236: loss = 94.125137\n",
      "237: loss = 94.066292\n",
      "238: loss = 94.017464\n",
      "239: loss = 93.993088\n",
      "240: loss = 93.909996\n",
      "241: loss = 93.896461\n",
      "242: loss = 93.859924\n",
      "243: loss = 93.754509\n",
      "244: loss = 93.788979\n",
      "245: loss = 93.682617\n",
      "246: loss = 93.709808\n",
      "247: loss = 93.629333\n",
      "248: loss = 93.599220\n",
      "249: loss = 93.541542\n",
      "\n",
      "train error: 1.40% (14/1000)\n",
      "test error: 2.10% 21/1000 \n",
      "\n",
      "Whole plot of the loss:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4a98db4320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Main Function\n",
    "\n",
    "# Generation of the train and test set\n",
    "train_input, train_target = generate_disc_set(1000)\n",
    "test_input, test_target = generate_disc_set(1000)\n",
    "\n",
    "mean, std = train_input.mean(), train_input.std()     # normailzation of the data\n",
    "train_input.sub_(mean).div_(std)\n",
    "test_input.sub_(mean).div_(std);\n",
    "\n",
    "# Training parameters\n",
    "mini_batch_size = 100\n",
    "nb_epochs = 250\n",
    "learning_rate = 1e-3\n",
    "standard_deviation = 0.1\n",
    "hidden_layer = 25\n",
    "\n",
    "# initilization of the model and criterion\n",
    "model = Sequential(Linear(2, hidden_layer, standard_deviation), Tanh(), \n",
    "                   Linear(hidden_layer, hidden_layer, standard_deviation), ReLU(), \n",
    "                   Linear(hidden_layer, 2, standard_deviation))\n",
    "criterion = MSEloss()\n",
    "\n",
    "# training of the model\n",
    "sum_loss = train_model(model, criterion, train_input, train_target, mini_batch_size, learning_rate, nb_epochs)\n",
    "\n",
    "# computing of the number of error\n",
    "nb_test_errors = compute_nb_errors(model, test_input, test_target, mini_batch_size)\n",
    "nb_train_errors = compute_nb_errors(model, train_input, train_target, mini_batch_size)\n",
    "\n",
    "# visualization\n",
    "print('\\ntrain error: {:0.2f}% ({:d}/{:d})'.format((100 * nb_train_errors) / train_input.size(0),   # number of errors on test\n",
    "                                                      nb_train_errors, train_input.size(0)))\n",
    "print('test error: {:0.2f}% {:d}/{:d} \\n'.format((100 * nb_test_errors) / test_input.size(0),      # number of errors on test\n",
    "                                                      nb_test_errors, test_input.size(0)))\n",
    "\n",
    "print('Whole plot of the loss:')\n",
    "plt.plot(range(nb_epochs), sum_loss.numpy())   # plots the loss at each iteration\n",
    "plt.xlabel('epochs nÂ°')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
