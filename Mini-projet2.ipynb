{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBERS = '0123456789' \n",
    "def increment(s): # function taken from here:\n",
    "    out = ''      # https://codegolf.stackexchange.com/questions/38033/increment-every-number-in-a-string\n",
    "\n",
    "    number = ''\n",
    "    for c in s:\n",
    "        if c in NUMBERS:\n",
    "            number += c\n",
    "        else:\n",
    "            if number != '':\n",
    "                out += str(int(number) + 1)\n",
    "                number = ''\n",
    "            out += c\n",
    "\n",
    "    if number != '':\n",
    "        out += str(int(number) + 1)\n",
    "        number = ''\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_disc_set(nb):\n",
    "    input = FloatTensor(nb, 2).uniform_(0, 1)\n",
    "    target = torch.norm(input,2,1) < math.sqrt(1/(2*math.pi))\n",
    "    \n",
    "    return input, target.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Parameter\n",
    "\n",
    "class Parameter():\n",
    "    def __init__(self, name, tensor, gradient):\n",
    "        self.name = name\n",
    "        self.data = tensor\n",
    "        self.grad = gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple structure for a Module\n",
    "\n",
    "class Module ( Parameter ) :\n",
    "    def __init__(self):\n",
    "        super(Parameter, self).__init__()\n",
    "        self.param = []\n",
    "        \n",
    "    def forward ( self , * input ) :\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward ( self , * gradwrtoutput ) :\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def init_parameters ( self ):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def add_parameter( self , parameter ):\n",
    "        if parameter.__class__.__name__ == 'Parameter':\n",
    "            self.param.append((parameter.name, parameter.data, parameter.grad))\n",
    "        elif parameter.__class__.__name__ == 'list':                        \n",
    "            if parameter != []:\n",
    "                self.param.append(parameter)\n",
    "                    \n",
    "    def zero_grad( self ):\n",
    "        for i in range(len(self.param)):\n",
    "            for j in range(len(self.param[i])):\n",
    "                self.param[i][j][2][:] = 0\n",
    "                    \n",
    "    def optimizer ( self , lr=1e-2 ):\n",
    "        for i in range(len(self.param)):\n",
    "            for j in range(len(self.param[i])):\n",
    "                self.param[i][j][1][:] -= lr * self.param[i][j][2][:]\n",
    "                \n",
    "    def parameters ( self ):\n",
    "        return self.param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module for MSEloss\n",
    "\n",
    "class MSEloss( Module ):\n",
    "    def __init__(self):\n",
    "        super(MSEloss, self).__init__()\n",
    "    def forward ( self , input, target ):\n",
    "        return input.sub(target.view(-1, 1)).pow(2).sum()    \n",
    "    def backward ( self , input, target ):\n",
    "        return 2*(input.sub(target.view(-1, 1)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module ReLU\n",
    "\n",
    "class ReLU( Module ):\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "        self.name = 'ReLU'\n",
    "    def forward( self, input ):\n",
    "        return input.clamp(min = 0)\n",
    "    def backward( self, input ):\n",
    "        input[input <= 0] = 0\n",
    "        input[input > 0] = 1\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module Tanh\n",
    "\n",
    "class Tanh( Module ):\n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "        self.name = 'Tanh'\n",
    "    def forward( self , input ):\n",
    "        return input.tanh()\n",
    "    def backward( self, input ):\n",
    "        return 4 * (input.exp() + input.mul(-1).exp()).pow(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module linear\n",
    "\n",
    "class Linear( Module ):\n",
    "    \n",
    "    Linear_counter = 0\n",
    "    \n",
    "    def __init__(self, input_features, output_features, eps=1e-2):\n",
    "        super(Linear, self).__init__()\n",
    "        self.name = 'Linear'\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "        self.init_parameters(input_features, output_features, eps)\n",
    "        Linear.Linear_counter +=1\n",
    "        \n",
    "    def init_parameters ( self, input_features, output_features, eps ):\n",
    "        weigths_name = 'weights0'\n",
    "        bias_name = 'bias0'\n",
    "        for i in range(self.Linear_counter):\n",
    "            weigths_name = increment(weigths_name)\n",
    "            bias_name = increment(bias_name)\n",
    "        self.weights = Parameter(weigths_name, torch.Tensor(input_features, output_features),\n",
    "                                 torch.Tensor(input_features, output_features))\n",
    "        self.bias = Parameter(bias_name, torch.Tensor(output_features), torch.Tensor(output_features))\n",
    "        self.weights.data.normal_(0, eps)\n",
    "        self.weights.data[self.weights.data < 0] *= -1 # to get positive values\n",
    "        self.weights.grad.zero_()\n",
    "        self.bias.grad.zero_()\n",
    "        self.bias.data.normal_(0, eps)\n",
    "        self.bias.data[self.bias.data < 0] *= -1 \n",
    "        self.add_parameter(self.weights)\n",
    "        self.add_parameter(self.bias)\n",
    "        \n",
    "    def forward( self , input):\n",
    "        output = input.mm(self.weights.data)\n",
    "        output += self.bias.data\n",
    "        return output\n",
    "    \n",
    "    def backward( self, ds , input ):\n",
    "        dx = ds.mm(self.weights.data.t())\n",
    "        dw = input.t().mm(ds)\n",
    "        db = ds.t().mm(torch.ones(ds.size(0),1))\n",
    "        return dx, dw, db\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module Sequential\n",
    "\n",
    "class Sequential( Module ):\n",
    "    def __init__(self, *args):\n",
    "        super(Sequential, self).__init__()\n",
    "        Linear.Linear_counter = 0\n",
    "        self.module_nb = len(args)\n",
    "        self.fc = [None] * self.module_nb\n",
    "        self.x = [None] * (self.module_nb + 1)\n",
    "        for id, module in enumerate(args):\n",
    "            self.fc[id] = module\n",
    "            self.init_parameters(id)\n",
    "        \n",
    "    def init_parameters ( self, id ):\n",
    "        self.add_parameter( self.fc[id].parameters() )\n",
    "        \n",
    "    def forward( self, input):\n",
    "        self.x[0] = input\n",
    "        for i in range(1, self.module_nb + 1):\n",
    "            self.x[i] = self.fc[i-1].forward(self.x[i-1])\n",
    "        return self.x[self.module_nb]\n",
    "    \n",
    "    def backward( self, dloss):\n",
    "        dx = [None] * (self.module_nb + 1)\n",
    "        dx[self.module_nb] = dloss\n",
    "        for i in range(1, self.module_nb + 1):\n",
    "            j = self.module_nb - i\n",
    "            if self.fc[j].parameters() == []:\n",
    "                dx[j] = self.fc[j].backward(self.x[j]) * dx[j+1]\n",
    "            else:\n",
    "                dx[j], dw, db = self.fc[j].backward( dx[j+1], self.x[j])\n",
    "                self.fc[j].weights.grad.add_(dw)\n",
    "                self.fc[j].bias.grad.add_(db)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net 2 layers\n",
    "\n",
    "Linear.Linear_counter = 0\n",
    "\n",
    "class Net2( Module ):\n",
    "    def __init__( self , hidden_layer):\n",
    "        super(Net2, self).__init__()\n",
    "        self.fc1 = Linear(2, hidden_layer)\n",
    "        self.fc2 = ReLU()\n",
    "        self.fc3 = Linear(hidden_layer, 1)\n",
    "        self.init_parameters()\n",
    "        \n",
    "    def init_parameters ( self ):\n",
    "        self.add_parameter( self.fc1.parameters() )\n",
    "        self.add_parameter( self.fc2.parameters() )\n",
    "        self.add_parameter( self.fc3.parameters() )\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.x0 = input\n",
    "        s1 = self.fc1.forward( input )\n",
    "        self.s1 = s1\n",
    "        x1 = self.fc2.forward( s1 )\n",
    "        self.x1 = x1\n",
    "        s2 = self.fc3.forward( x1 )\n",
    "        self.s2 = s2\n",
    "        x2 = self.fc2.forward( s2 )\n",
    "        self.x2 = x2\n",
    "        return x2\n",
    "    \n",
    "    def backward(self, dloss):\n",
    "        dx2 = dloss\n",
    "        ds2 = self.fc2.backward(self.s2) * dx2\n",
    "        dx1, dw3, db3 = self.fc3.backward( ds2 , self.x1 )\n",
    "        self.fc3.weights.grad.add_(dw3)\n",
    "        self.fc3.bias.grad.add_(db3)\n",
    "        \n",
    "        ds1 = self.fc2.backward(self.s1) * dx1\n",
    "        \n",
    "        dx0, dw1, db1 = self.fc1.backward( ds1 , self.x0 )\n",
    "\n",
    "        self.fc1.weights.grad.add_(dw1)\n",
    "        self.fc1.bias.grad.add_(db1)\n",
    "        \n",
    "        return dx0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net 4 layers\n",
    "\n",
    "Linear.Linear_counter = 0\n",
    "\n",
    "class Net4( Module ):\n",
    "    def __init__(self, hidden_layer):\n",
    "        super(Net4, self).__init__()\n",
    "        self.fc1 = Linear(2, 2*hidden_layer)\n",
    "        self.fc2 = ReLU()\n",
    "        self.fc3 = Linear(2*hidden_layer, hidden_layer)\n",
    "        self.fc4 = Linear(hidden_layer, 1)\n",
    "        self.init_parameters()\n",
    "        \n",
    "    def init_parameters ( self ):\n",
    "        self.add_parameter( self.fc1.parameters() )\n",
    "        self.add_parameter( self.fc2.parameters() )\n",
    "        self.add_parameter( self.fc3.parameters() )\n",
    "        self.add_parameter( self.fc4.parameters() )\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x0 = x\n",
    "        x = self.fc1.forward( x )\n",
    "        self.s1 = x\n",
    "        x = self.fc2.forward( x )\n",
    "        self.x1 = x\n",
    "        x = self.fc3.forward( x )\n",
    "        self.s2 = x\n",
    "        x = self.fc2.forward( x )\n",
    "        self.x2 = x\n",
    "        x = self.fc4.forward( x )\n",
    "        self.s3 = x\n",
    "        x = self.fc2.forward( x )\n",
    "        self.x3 = x\n",
    "        return x\n",
    "    \n",
    "    def backward(self, dloss):\n",
    "        dx3 = dloss\n",
    "        ds3 = self.fc2.backward(self.s3) * dx3\n",
    "        dx2, dw4, db4 = self.fc4.backward( ds3, self.x2 )\n",
    "        self.fc4.weights.grad.add_(dw4)\n",
    "        self.fc4.bias.grad.add_(db4)\n",
    "        ds2 = self.fc2.backward(self.s2) * dx2\n",
    "        dx1, dw3, db3 = self.fc3.backward( ds2 , self.x1 )\n",
    "        self.fc3.weights.grad.add_(dw3)\n",
    "        self.fc3.bias.grad.add_(db3)\n",
    "        \n",
    "        ds1 = self.fc2.backward(self.s1) * dx1\n",
    "        \n",
    "        dx0, dw1, db1 = self.fc1.backward( ds1 , self.x0 )\n",
    "\n",
    "        self.fc1.weights.grad.add_(dw1)\n",
    "        self.fc1.bias.grad.add_(db1)\n",
    "        \n",
    "        return dx0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, mini_batch_size, lr = 1e-1):\n",
    "    nb_epochs = 30\n",
    "    criterion = MSEloss()\n",
    "    \n",
    "    for e in range(0, nb_epochs):\n",
    "        sum_loss = 0\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model.forward(train_input.narrow(0, b, mini_batch_size))\n",
    "            loss = criterion.forward(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            dloss = criterion.backward(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            sum_loss += loss\n",
    "            model.zero_grad()\n",
    "            dx0 = model.backward(dloss)\n",
    "            model.optimizer(lr)\n",
    "        print(e, sum_loss)\n",
    "        #param = model.parameters()\n",
    "        #print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, data_input, data_target, mini_batch_size):\n",
    "\n",
    "    total_nb_errors = 0\n",
    "\n",
    "    for b in range(0, data_input.size(0), mini_batch_size):\n",
    "        output = model.forward(data_input.narrow(0, b, mini_batch_size))\n",
    "        _, predicted_classes = torch.max(output, 1)\n",
    "        for k in range(0, mini_batch_size):\n",
    "            if data_target[b + k] != predicted_classes[k]:\n",
    "                total_nb_errors += 1\n",
    "\n",
    "    return total_nb_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dave/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:33: UserWarning: other is not broadcastable to self, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 112.51492991075429\n",
      "1 109.21502580522792\n",
      "2 107.01145547628403\n",
      "3 105.53978169872425\n",
      "4 104.55675716139376\n",
      "5 103.89999046875164\n",
      "6 103.46108913887292\n",
      "7 103.16769059281796\n",
      "8 102.97147973999381\n",
      "9 102.8401978770271\n",
      "10 102.75231356546283\n",
      "11 102.69343763124198\n",
      "12 102.6539566423744\n",
      "13 102.62745896726847\n",
      "14 102.60964890755713\n",
      "15 102.59765906073153\n",
      "16 102.58957846835256\n",
      "17 102.58411366678774\n",
      "18 102.5804095454514\n",
      "19 102.57789393700659\n",
      "20 102.57617613393813\n",
      "21 102.57499998714775\n",
      "22 102.57419175002724\n",
      "23 102.57363052573055\n",
      "24 102.57323851343244\n",
      "25 102.57296390086412\n",
      "26 102.57276741974056\n",
      "27 102.57262703590095\n",
      "28 102.57252726703882\n",
      "29 102.57245516311377\n",
      "129\n",
      "test error Net 12.90%% 129/1000\n"
     ]
    }
   ],
   "source": [
    "train_input, train_target = generate_disc_set(1000)\n",
    "test_input, test_target = generate_disc_set(1000)\n",
    "\n",
    "mean, std = train_input.mean(), train_input.std()\n",
    "\n",
    "train_input.sub_(mean).div_(std)\n",
    "test_input.sub_(mean).div_(std);\n",
    "\n",
    "mini_batch_size = 100\n",
    "\n",
    "model = Sequential(Linear(2, 5), ReLU(), Linear(5, 10), ReLU(), Linear(10, 20), ReLU(), Linear(20, 50), \n",
    "                   ReLU(), Linear(50, 20), ReLU(), Linear(20, 10), ReLU(), Linear(10, 5), ReLU(), Linear(5, 1), ReLU())\n",
    "train_model(model, train_input, train_target, mini_batch_size, 1e-4)\n",
    "nb_test_errors = compute_nb_errors(model, test_input, test_target, mini_batch_size)\n",
    "print(nb_test_errors)\n",
    "print('test error Net {:0.2f}%% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                      nb_test_errors, test_input.size(0)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
