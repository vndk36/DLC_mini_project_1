{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is distributed under BSD 3-Clause license\n",
    "\n",
    "import torch\n",
    "import numpy\n",
    "import os\n",
    "import errno\n",
    "\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "from six.moves import urllib\n",
    "\n",
    "#loss_plot = []\n",
    "\n",
    "def tensor_from_file(root, filename,\n",
    "                     base_url = 'https://documents.epfl.ch/users/f/fl/fleuret/www/data/bci'):\n",
    "\n",
    "    file_path = os.path.join(root, filename)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        try:\n",
    "            os.makedirs(root)\n",
    "        except OSError as e:\n",
    "            if e.errno == errno.EEXIST:\n",
    "                pass\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        url = base_url + '/' + filename\n",
    "\n",
    "        print('Downloading ' + url)\n",
    "        data = urllib.request.urlopen(url)\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(data.read())\n",
    "\n",
    "    return torch.from_numpy(numpy.loadtxt(file_path))\n",
    "\n",
    "def load(root, train = True, download = True, one_khz = False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "\n",
    "        root (string): Root directory of dataset.\n",
    "\n",
    "        train (bool, optional): If True, creates dataset from training data.\n",
    "\n",
    "        download (bool, optional): If True, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "\n",
    "        one_khz (bool, optional): If True, creates dataset from the 1000Hz data instead\n",
    "            of the default 100Hz.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    nb_electrodes = 28\n",
    "\n",
    "    if train:\n",
    "\n",
    "        if one_khz:\n",
    "            dataset = tensor_from_file(root, 'sp1s_aa_train_1000Hz.txt')\n",
    "        else:\n",
    "            dataset = tensor_from_file(root, 'sp1s_aa_train.txt')\n",
    "\n",
    "        input = dataset.narrow(1, 1, dataset.size(1) - 1)\n",
    "        input = input.float().view(input.size(0), nb_electrodes, -1)\n",
    "        target = dataset.narrow(1, 0, 1).clone().view(-1).long() #changer le type suivant le loss criterion\n",
    "\n",
    "    else:\n",
    "\n",
    "        if one_khz:\n",
    "            input = tensor_from_file(root, 'sp1s_aa_test_1000Hz.txt')\n",
    "        else:\n",
    "            input = tensor_from_file(root, 'sp1s_aa_test.txt')\n",
    "        target = tensor_from_file(root, 'labels_data_set_iv.txt')\n",
    "\n",
    "        input = input.float().view(input.size(0), nb_electrodes, -1)\n",
    "        target = target.view(-1).long() #changer le type suivant le loss criterion\n",
    "\n",
    "    return input, target\n",
    "            \n",
    "#################################################################\n",
    "\n",
    "def compute_nb_errors(model, data_input, data_target, mini_batch_size):\n",
    "\n",
    "    nb_data_errors = 0\n",
    "\n",
    "    for b in range(0, data_input.size(0), mini_batch_size):\n",
    "        output = model(data_input.narrow(0, b, mini_batch_size))\n",
    "        _, predicted_classes = torch.max(output.data, 1)\n",
    "        for k in range(0, mini_batch_size):\n",
    "            if data_target.data[b + k] != predicted_classes[k]:\n",
    "                nb_data_errors = nb_data_errors + 1\n",
    "\n",
    "    return nb_data_errors\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target = load(\"data\",True, False)\n",
    "test_input, test_target = load(\"data\",False, False)\n",
    "train_input, train_target, test_input, test_target = \\\n",
    "Variable(train_input.narrow(0, 0,300)), Variable(train_target.narrow(0,0,300)), Variable(test_input), Variable(test_target)\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Plot one input to a visual plot to be able to see what is going on!\n",
    "# plt.plot([x], y, ....)\n",
    "\n",
    "#plt.plot(range(0,50),train_input[4,4,:].data.numpy())\n",
    "#plt.show()\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "\n",
    "def train_model(model, train_input, train_target, loss_plot, nb_epochs, batch_size):\n",
    "    lr = 1e-3\n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr = lr)\n",
    "    #optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "    for e in range(0, nb_epochs):\n",
    "        permutation = torch.randperm(train_input.size()[0])\n",
    "        sum_loss = 0\n",
    "        for b in range(0, train_input.size(0), batch_size):\n",
    "            indices = permutation[b:b+batch_size]\n",
    "            batch_input, batch_target = train_input[indices], train_target[indices]\n",
    "            \n",
    "            output = model(batch_input)\n",
    "            loss = criterion(output, batch_target)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss = sum_loss + loss\n",
    "            \n",
    "        #print(\"The loss is :\"+str(loss)+\" for epoch :\"+str(e))\n",
    "        loss_plot.append(sum_loss.data.numpy())\n",
    "        print(str(int((e/nb_epochs)*100))+\"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#################################################################\n",
    "class NetLin(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(NetLin, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*50,2000)\n",
    "        self.fc2 = nn.Linear(2000, 1000)\n",
    "        self.fc3 = nn.Linear(1000, 100)\n",
    "        self.fc4 = nn.Linear(100, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x.view(-1,x.data.size()[1]* x.data.size()[2])))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "####################################################################################################################\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(28, 64, kernel_size=5)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3)\n",
    "        self.conv3 = nn.Conv1d(128, 128, kernel_size=2)\n",
    "        self.fc1 = nn.Linear(128*3, nb_hidden*3)\n",
    "        self.fc2 = nn.Linear(nb_hidden*3, nb_hidden*2)\n",
    "        self.fc3 = nn.Linear(nb_hidden*2, nb_hidden)\n",
    "        self.fc4 = nn.Linear(nb_hidden, 2)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.dropout(x)\n",
    "        x = F.relu(F.max_pool1d(self.conv1(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(F.max_pool1d(self.conv2(x), kernel_size=3, stride=3))\n",
    "        x = F.relu(F.max_pool1d(self.conv3(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1,x.data.size()[1]* x.data.size()[2])))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "#model = nn.Sequential(\n",
    "    #nn.Conv1d(28, 32, kernel_size=5), \n",
    "    #nn.ReLU(),\n",
    "    #nn.Conv1d(56, 56, kernel_size=4, stride = 2),\n",
    "    #nn.ReLU(),\n",
    "    #nn.MaxPool1d(kernel_size = 5, stride = 3),\n",
    "    #nn.ReLU(),\n",
    "    #nn.Conv1d(56,56, kernel_size = 4, stride = 1),\n",
    "    #nn.ReLU(),\n",
    "    #nn.Linear(32*46,79),\n",
    "    #nn.ReLU(),\n",
    "    #nn.Linear(79, 2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the right model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%\n",
      "1%\n",
      "2%\n",
      "3%\n",
      "4%\n",
      "5%\n",
      "6%\n",
      "7%\n",
      "8%\n",
      "9%\n",
      "10%\n",
      "11%\n",
      "12%\n",
      "13%\n",
      "14%\n",
      "15%\n",
      "16%\n",
      "17%\n",
      "18%\n",
      "19%\n",
      "20%\n",
      "21%\n",
      "22%\n",
      "23%\n",
      "24%\n",
      "25%\n",
      "26%\n",
      "27%\n",
      "28%\n",
      "28%\n",
      "30%\n",
      "31%\n",
      "32%\n",
      "33%\n",
      "34%\n",
      "35%\n",
      "36%\n",
      "37%\n",
      "38%\n",
      "39%\n",
      "40%\n",
      "41%\n",
      "42%\n",
      "43%\n",
      "44%\n",
      "45%\n",
      "46%\n",
      "47%\n",
      "48%\n",
      "49%\n",
      "50%\n",
      "51%\n",
      "52%\n",
      "53%\n",
      "54%\n",
      "55%\n",
      "56%\n",
      "56%\n",
      "57%\n",
      "59%\n",
      "60%\n",
      "61%\n",
      "62%\n",
      "63%\n"
     ]
    }
   ],
   "source": [
    "# test multiple prior init\n",
    "nb_epochs =  100\n",
    "batch_size = 50\n",
    "\n",
    "\n",
    "nb_training = 3\n",
    "\n",
    "for i in range(0,nb_training):\n",
    "    for p in model.parameters(): p.data.normal_(0, 0.01)\n",
    "    loss_plot = []\n",
    "    train_model(model, train_input, train_target, loss_plot, nb_epochs, batch_size = batch_size)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(' train_error {:.02f}% test_error {:.02f}%'.format(\n",
    "                compute_nb_errors(model, train_input, train_target, 20) / train_input.size(0) * 100,\n",
    "                compute_nb_errors(model, test_input, test_target, 50) / test_input.size(0) * 100))\n",
    "    #loss_plot\n",
    "    for p in model.parameters(): p.data.normal_(0, 0.01)\n",
    "    plt.plot(range(nb_epochs),loss_plot)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
